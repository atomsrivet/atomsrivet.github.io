<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 A map of decoy influence in multialternative economic choice | Context Dependencies in Decision Making</title>
  <meta name="description" content="2 A map of decoy influence in multialternative economic choice | Context Dependencies in Decision Making" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="2 A map of decoy influence in multialternative economic choice | Context Dependencies in Decision Making" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 A map of decoy influence in multialternative economic choice | Context Dependencies in Decision Making" />
  
  
  

<meta name="author" content="Tsvetomira Dumbalska" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="distr.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="templates/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#optimality"><i class="fa fa-check"></i><b>1.1</b> Optimal Decisions</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#optimality-perc"><i class="fa fa-check"></i><b>1.1.1</b> Optimality in Perception</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#optimality-econ"><i class="fa fa-check"></i><b>1.1.2</b> Optimality in Economics</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#context-dependencies"><i class="fa fa-check"></i><b>1.2</b> Context Dependencies</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#perceptual-decisions"><i class="fa fa-check"></i><b>1.2.1</b> Perceptual Decisions</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#economic-decisions"><i class="fa fa-check"></i><b>1.2.2</b> Economic Decisions</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#parallels"><i class="fa fa-check"></i><b>1.2.3</b> Parallels</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#the-underpinnings-of-context-dependent-choice"><i class="fa fa-check"></i><b>1.3</b> The Underpinnings of Context-Dependent Choice</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#neural-constraints-efficiency"><i class="fa fa-check"></i><b>1.3.1</b> Neural Constraints &amp; Efficiency</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#behavioral-relevance"><i class="fa fa-check"></i><b>1.3.2</b> Behavioral Relevance</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#thesis-outline"><i class="fa fa-check"></i><b>1.4</b> Thesis Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="decoy.html"><a href="decoy.html"><i class="fa fa-check"></i><b>2</b> A map of decoy influence in multialternative economic choice</a>
<ul>
<li class="chapter" data-level="2.1" data-path="decoy.html"><a href="decoy.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="decoy.html"><a href="decoy.html#experiment"><i class="fa fa-check"></i><b>2.2</b> Experiment</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="decoy.html"><a href="decoy.html#methods"><i class="fa fa-check"></i><b>2.2.1</b> Methods</a></li>
<li class="chapter" data-level="2.2.2" data-path="decoy.html"><a href="decoy.html#results"><i class="fa fa-check"></i><b>2.2.2</b> Results</a></li>
<li class="chapter" data-level="2.2.3" data-path="decoy.html"><a href="decoy.html#interim-discussion"><i class="fa fa-check"></i><b>2.2.3</b> Interim Discussion</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="decoy.html"><a href="decoy.html#computational-modeling"><i class="fa fa-check"></i><b>2.3</b> Computational Modeling</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="decoy.html"><a href="decoy.html#adaptive-gain-model"><i class="fa fa-check"></i><b>2.3.1</b> Adaptive Gain Model</a></li>
<li class="chapter" data-level="2.3.2" data-path="decoy.html"><a href="decoy.html#model-comparisons"><i class="fa fa-check"></i><b>2.3.2</b> Model Comparisons</a></li>
<li class="chapter" data-level="2.3.3" data-path="decoy.html"><a href="decoy.html#charting-normalization-models-of-decoy-influence"><i class="fa fa-check"></i><b>2.3.3</b> Charting Normalization Models of Decoy Influence</a></li>
<li class="chapter" data-level="2.3.4" data-path="decoy.html"><a href="decoy.html#interim-discussion-1"><i class="fa fa-check"></i><b>2.3.4</b> Interim Discussion</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="decoy.html"><a href="decoy.html#discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
<li class="chapter" data-level="2.5" data-path="decoy.html"><a href="decoy.html#conclusion"><i class="fa fa-check"></i><b>2.5</b> Conclusion</a></li>
<li class="chapter" data-level="2.6" data-path="decoy.html"><a href="decoy.html#acknowledgements"><i class="fa fa-check"></i><b>2.6</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="distr.html"><a href="distr.html"><i class="fa fa-check"></i><b>3</b> How do perceptual distractors distract?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="distr.html"><a href="distr.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="distr.html"><a href="distr.html#experiment-1"><i class="fa fa-check"></i><b>3.2</b> Experiment 1</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="distr.html"><a href="distr.html#methods-3"><i class="fa fa-check"></i><b>3.2.1</b> Methods</a></li>
<li class="chapter" data-level="3.2.2" data-path="distr.html"><a href="distr.html#results-3"><i class="fa fa-check"></i><b>3.2.2</b> Results</a></li>
<li class="chapter" data-level="3.2.3" data-path="distr.html"><a href="distr.html#interim-discussion-2"><i class="fa fa-check"></i><b>3.2.3</b> Interim Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="distr.html"><a href="distr.html#experiment-2"><i class="fa fa-check"></i><b>3.3</b> Experiment 2</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="distr.html"><a href="distr.html#methods-4"><i class="fa fa-check"></i><b>3.3.1</b> Methods</a></li>
<li class="chapter" data-level="3.3.2" data-path="distr.html"><a href="distr.html#results-4"><i class="fa fa-check"></i><b>3.3.2</b> Results</a></li>
<li class="chapter" data-level="3.3.3" data-path="distr.html"><a href="distr.html#interim-discussion-2"><i class="fa fa-check"></i><b>3.3.3</b> Interim Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="distr.html"><a href="distr.html#experiment-3"><i class="fa fa-check"></i><b>3.4</b> Experiment 3</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="distr.html"><a href="distr.html#methods-5"><i class="fa fa-check"></i><b>3.4.1</b> Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="distr.html"><a href="distr.html#results-5"><i class="fa fa-check"></i><b>3.4.2</b> Results</a></li>
<li class="chapter" data-level="3.4.3" data-path="distr.html"><a href="distr.html#interim-discussion-3"><i class="fa fa-check"></i><b>3.4.3</b> Interim Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="distr.html"><a href="distr.html#computational-modeling-1"><i class="fa fa-check"></i><b>3.5</b> Computational Modeling</a></li>
<li class="chapter" data-level="3.6" data-path="distr.html"><a href="distr.html#discussion-1"><i class="fa fa-check"></i><b>3.6</b> Discussion</a></li>
<li class="chapter" data-level="3.7" data-path="distr.html"><a href="distr.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
<li class="chapter" data-level="3.8" data-path="distr.html"><a href="distr.html#acknowledgements-1"><i class="fa fa-check"></i><b>3.8</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cat.html"><a href="cat.html"><i class="fa fa-check"></i><b>4</b> Context-dependent categorization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="cat.html"><a href="cat.html#introduction-3"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="cat.html"><a href="cat.html#experiment-1-1"><i class="fa fa-check"></i><b>4.2</b> Experiment 1</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="cat.html"><a href="cat.html#methods-6"><i class="fa fa-check"></i><b>4.2.1</b> Methods</a></li>
<li class="chapter" data-level="4.2.2" data-path="cat.html"><a href="cat.html#results-6"><i class="fa fa-check"></i><b>4.2.2</b> Results</a></li>
<li class="chapter" data-level="4.2.3" data-path="cat.html"><a href="cat.html#interim-discussion-4"><i class="fa fa-check"></i><b>4.2.3</b> Interim Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="cat.html"><a href="cat.html#experiment-2-1"><i class="fa fa-check"></i><b>4.3</b> Experiment 2</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="cat.html"><a href="cat.html#methods-7"><i class="fa fa-check"></i><b>4.3.1</b> Methods</a></li>
<li class="chapter" data-level="4.3.2" data-path="cat.html"><a href="cat.html#results-7"><i class="fa fa-check"></i><b>4.3.2</b> Results</a></li>
<li class="chapter" data-level="4.3.3" data-path="cat.html"><a href="cat.html#interim-discussion-5"><i class="fa fa-check"></i><b>4.3.3</b> Interim Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="cat.html"><a href="cat.html#discussion-2"><i class="fa fa-check"></i><b>4.4</b> Discussion</a></li>
<li class="chapter" data-level="4.5" data-path="cat.html"><a href="cat.html#conclusion-2"><i class="fa fa-check"></i><b>4.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="discussion-3.html"><a href="discussion-3.html"><i class="fa fa-check"></i><b>5</b> Discussion</a>
<ul>
<li class="chapter" data-level="5.1" data-path="discussion-3.html"><a href="discussion-3.html#overview-of-results"><i class="fa fa-check"></i><b>5.1</b> Overview of Results</a></li>
<li class="chapter" data-level="5.2" data-path="discussion-3.html"><a href="discussion-3.html#interpretation"><i class="fa fa-check"></i><b>5.2</b> Interpretation</a></li>
<li class="chapter" data-level="5.3" data-path="discussion-3.html"><a href="discussion-3.html#zooming-out"><i class="fa fa-check"></i><b>5.3</b> Zooming Out</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="discussion-3.html"><a href="discussion-3.html#context-dependence-in-the-wild"><i class="fa fa-check"></i><b>5.3.1</b> Context Dependence in the Wild</a></li>
<li class="chapter" data-level="5.3.2" data-path="discussion-3.html"><a href="discussion-3.html#context-dependence-as-a-tool"><i class="fa fa-check"></i><b>5.3.2</b> Context Dependence as a Tool</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="discussion-3.html"><a href="discussion-3.html#conclusion-3"><i class="fa fa-check"></i><b>5.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><p>Context Dependencies in Decision Making</p></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decoy" class="section level1" number="2">
<h1><span class="header-section-number">2</span> A map of decoy influence in multialternative economic choice</h1>
<p>
<em>When choosing between two alternatives, the addition of a third inferior option should not affect choices. In practice, however, even though the dispreferred decoy good is rarely chosen, it can systematically sway preference towards one of the two original alternatives. Previous work has identified three classic decoy biases, known as the attraction, similarity and compromise effects, which arise during choices between economic alternatives defined by two attributes. However, the reliability, interrelationship, and computational origin of these three biases has been controversial. Here, a large cohort of human participants made incentive-compatible choices among assets that varied in price and quality. Instead of focusing on the three classic effects, we sampled decoy stimuli exhaustively across bidimensional multiattribute space and constructed a full map of decoy influence on choices between two otherwise preferred target items. Our analysis revealed that the decoy influence map was highly structured even beyond the three classic biases. We identified a very simple model that can fully reproduce the decoy influence map and capture its variability in individual participants. This model reveals that the three decoy effects are not distinct phenomena but are all special cases of a more general principle, by which attribute values are repulsed away from the context provided by rival options. The model helps understand why the biases are typically correlated across participants and allows us to validate a new prediction about their interrelationship. This work helps clarify the origin of three of the most widely studied biases in human decision-making.</em></p>
<div id="introduction-1" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>Optimal decisions should be driven solely by information that is relevant for the choice. When deliberating among more than two options (“multialternative” choices) this means ignoring those alternatives that are inferior or unavailable. Hence, the choice between two consumer goods should not be affected when an unaffordable third option is introduced. Similarly, voting preferences between two electoral candidates should not be changed when a third contender with more dubious merit enters the race. This normative principle, which is enshrined in the axiom of regularity <span class="citation">(<a href="#ref-block1960" role="doc-biblioref">Block and Marschak 1960</a>; <a href="#ref-rieskamp2006" role="doc-biblioref">Rieskamp, Busemeyer, and Mellers 2006</a>)</span>, has been of significant and long-standing interest to behavioral scientists as it is robustly violated by a variety of animals including humans <span class="citation">(<a href="#ref-tversky1972a" role="doc-biblioref">A. Tversky 1972</a>; <a href="#ref-huber1982" role="doc-biblioref">J. Huber, Payne, and Puto 1982</a>; <a href="#ref-simonson1989" role="doc-biblioref">Simonson 1989</a>)</span>, monkeys <span class="citation">(<a href="#ref-parrish2015" role="doc-biblioref">Parrish, Evans, and Beran 2015</a>)</span>, amphibians <span class="citation">(<a href="#ref-lea2015" role="doc-biblioref">Lea and Ryan 2015</a>)</span>, invertebrates <span class="citation">(<a href="#ref-shafir1994" role="doc-biblioref">Shafir 1994</a>)</span>, and even unicellular organisms <span class="citation">(<a href="#ref-latty2011" role="doc-biblioref">Latty and Beekman 2011</a>)</span>. The existing empirical evidence has indicated that where choice alternatives are characterized by two value dimensions (e.g. the price and the quality of a product or the likability and competence of a political candidate), the introduction of an irrelevant distractor item to the choice set leads to rich and stereotyped biases in decision-making. A key research goal in the fields of psychology and economics has been to identify a simple and elegant computational principle that can explain the biases provoked by an irrelevant “decoy” stimulus <span class="citation">(<a href="#ref-turner2018" role="doc-biblioref">Turner et al. 2018</a>)</span>.</p>
<p>The existing literature has focused on three decoy effects that can arise during ternary (three-way) choice among alternatives characterized by two independent and equally weighed attributes. The phenomena are illustrated in Fig. <a href="decoy.html#fig:decoy-illustr">2.1</a>. Consider for example a consumer who is choosing between three products that are each characterized by dimensions (attributes) of quality and economy. The axes in Fig. <a href="decoy.html#fig:decoy-illustr">2.1</a> are scaled such that these attributes are perfect substitutions in that the consumer will forego one unit of one attribute for one unit of the other. Two target items, A and B, lie on the line of isopreference which is perpendicular to the identity line. In other words, A is less expensive but lower quality than B, such that the consumer should be indifferent between these options. The empirical phenomena describe how preferences may be biased towards either A or B as a function of a third “decoy” item D that lies on or below the isopreference line. The consensus view holds that a bias towards A can be provoked by the inclusion of a decoy <span class="math inline">\(D_a\)</span> that it dominates, that is where A (but not B) is equivalent or superior on both dimensions (the attraction effect); that a bias towards A occurs in the presence of a more extreme decoy <span class="math inline">\(D_c\)</span> which is superior in quality but yet more expensive than A, making A the “compromise” option (the compromise effect); and that a bias towards A is incurred by a decoy <span class="math inline">\(D_s\)</span> which is similar to B in price and quality (the similarity effect, Fig. <a href="decoy.html#fig:decoy-illustr">2.1</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-illustr"></span>
<img src="figures/decoy-illustr.png" alt="Illustration of the attraction, compromise and similarity effects. A and B denote two equally preferred stimuli; A is strong on attribute $i$ but weak on attribute $j$ and vice versa. The introduction of decoy stimuli (rings; denoted $D_a$, $D_c$ and $D_s$) can bias preferences towards either A or B. The color of each ring signals the direction of the bias, e.g. for orange rings, A is preferred. Stimuli falling on the dashed line are equally preferred." width="50%" />
<p class="caption">
Figure 2.1: Illustration of the attraction, compromise and similarity effects. A and B denote two equally preferred stimuli; A is strong on attribute <span class="math inline">\(i\)</span> but weak on attribute <span class="math inline">\(j\)</span> and vice versa. The introduction of decoy stimuli (rings; denoted <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_c\)</span> and <span class="math inline">\(D_s\)</span>) can bias preferences towards either A or B. The color of each ring signals the direction of the bias, e.g. for orange rings, A is preferred. Stimuli falling on the dashed line are equally preferred.
</p>
</div>
<p>These three phenomena have been a major object of study in psychology and behavioral economics for several decades, with research interest reinvigorated in the last couple of decades after a landmark study that proposed the first unified computational account of the three classic decoy effects <span class="citation">(<a href="#ref-roe2001" role="doc-biblioref">Roe, Busemeyer, and Townsend 2001</a>)</span>. Since then, a variety of empirical results have been produced and a plethora of computational models have emerged. These include models that rely on loss aversion <span class="citation">(<a href="#ref-tsetsos2010" role="doc-biblioref">K. Tsetsos, Usher, and Chater 2010</a>)</span>, pairwise normalization <span class="citation">(<a href="#ref-landry2021" role="doc-biblioref">Landry and Webb 2021</a>)</span>, attentional weighing <span class="citation">(<a href="#ref-hotaling2010" role="doc-biblioref">Hotaling, Busemeyer, and Li 2010</a>; <a href="#ref-tsetsos2012" role="doc-biblioref">Konstantinos Tsetsos, Chater, and Usher 2012</a>; <a href="#ref-bhatia2013" role="doc-biblioref">Bhatia 2013</a>; <a href="#ref-trueblood2014" role="doc-biblioref">Trueblood, Brown, and Heathcote 2014</a>)</span>, lateral inhibition <span class="citation">(<a href="#ref-hotaling2010" role="doc-biblioref">Hotaling, Busemeyer, and Li 2010</a>)</span>, associative biases <span class="citation">(<a href="#ref-bhatia2013" role="doc-biblioref">Bhatia 2013</a>)</span>, power-law transformation of attribute values <span class="citation">(<a href="#ref-bhatia2013" role="doc-biblioref">Bhatia 2013</a>)</span>, sampling from memory <span class="citation">(<a href="#ref-noguchi2014" role="doc-biblioref">Noguchi and Stewart 2014</a>; <a href="#ref-bhui2018" role="doc-biblioref">Bhui and Gershman 2018</a>)</span>, or various other forms of reference-dependent computation <span class="citation">(<a href="#ref-soltani2012" role="doc-biblioref">Soltani, De Martino, and Camerer 2012</a>; <a href="#ref-li2018" role="doc-biblioref">V. Li et al. 2018</a>; <a href="#ref-natenzon2019" role="doc-biblioref">Natenzon 2019</a>; <a href="#ref-rigoli2019" role="doc-biblioref">Rigoli 2019</a>)</span>. However, there has been a notable lack of consensus about the computational principles that give rise to decoy effects <span class="citation">(<a href="#ref-turner2018" role="doc-biblioref">Turner et al. 2018</a>)</span>. There are a number of potential reasons for this, but here we focus on one limitation of past studies: most have tested for decoy effects by selecting fixed attribute values for <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_c\)</span> and <span class="math inline">\(D_s\)</span> and calculating for each the relative choice share (RCS) for target items A and B, with relative deviations from choice equilibrium signalling a bias indicative of the successful detection of a decoy effect. However, reducing the dimensionality of the data in this way (i.e. to 6 data points) makes it harder to distinguish theoretical accounts, as many models may mimic one another in successfully capturing the phenomena, so that comparisons among models are reduced to questions of a priori plausibility and parsimony. Relatedly, what defines a “decoy” of each class is typically left largely to the discretion of the researcher, who is free to choose a priori the values for <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_c\)</span> and <span class="math inline">\(D_s\)</span> – i.e. the space over which the attraction, compromise or similarity might occur. This issue, coupled with the fact that the effects are often studied in small participant cohorts, using diverse stimulus materials – consumer choices <span class="citation">(<a href="#ref-josiam1995" role="doc-biblioref">Josiam and Hobson 1995</a>)</span>, text-based vignettes <span class="citation">(<a href="#ref-yang2014" role="doc-biblioref">Yang and Lynn 2014</a>)</span>, or perceptual judgments <span class="citation">(<a href="#ref-trueblood2013" role="doc-biblioref">Trueblood et al. 2013</a>)</span> – has led to disagreement over the provenance and reliability of the three effects <span class="citation">(<a href="#ref-trueblood2012" role="doc-biblioref">Trueblood 2012</a>; <a href="#ref-frederick2014" role="doc-biblioref">Frederick, Lee, and Baskin 2014</a>; <a href="#ref-yang2014" role="doc-biblioref">Yang and Lynn 2014</a>; <a href="#ref-trueblood2015" role="doc-biblioref">Trueblood, Brown, and Heathcote 2015</a>)</span>.</p>
<p>In this chapter, we address these issues by reporting a large-scale (<span class="math inline">\(n&gt;200\)</span>), incentive-compatible study. We systematically mapped the decoy influence across attribute space, calculating the relative choice share <span class="math inline">\(RCS_{ij}\)</span> for each decoy with attribute values <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. This allowed us to explore the dimensionality of the data, with a view to asking whether a single principle can explain the ensemble of observed decoy effects. We found that a remarkably simple model, which draws on a computational framework based on the principle of divisive normalization, can capture the full decoy influence (RCS) map. Importantly, the model suggests that the three canonical decoy effects are not in fact distinct phenomena but rather fall naturally out of previously described dynamics of attraction and repulsion of decision values towards and away from a reference value given by the mean of available options.</p>
</div>
<div id="experiment" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Experiment</h2>
<div id="methods" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Methods</h3>
<div id="participants" class="section level4" number="2.2.1.1">
<h4><span class="header-section-number">2.2.1.1</span> Participants</h4>
<p>A total of 358 US-based participants were recruited via the platform Amazon Mechanical Turk to participate in a three-phase online study. All participants took part in the first phase (rating task), and those who passed a performance threshold (<span class="math inline">\(n=231\)</span>; see below) were invited to join the second and third parts of the study in separate testing sessions (choice task). Of these, 189 met our criteria for inclusion in the analysis, namely <span class="math inline">\(p&lt;0.001\)</span> of responding randomly during the choice task (binomial test). Phases 2 and 3 (choice task) were identical; phase 3 simply allowed us to gather more data (<span class="math inline">\(n=149\)</span> completed both phases 2 and 3). Data were collected in two distinct batches. In the first batch, we paid participants $4 for completing each phase, in addition to a performance-based bonus of up to $20 for the second and third part of the study (a maximum payment of $32). To reduce the dropout rate, in the second batch the base payment was increased to $5 and the bonuses increased to $12 and $18 in the second and third phases respectively (a maximum payment of $45). The study received ethical approval from the Central University Research Ethics Committee at the University of Oxford (approval reference number: R50750/RE001).</p>
</div>
<div id="experimental-tasks" class="section level4" number="2.2.1.2">
<h4><span class="header-section-number">2.2.1.2</span> Experimental Tasks</h4>
<div id="rating-task" class="section level5" number="2.2.1.2.1">
<h5><span class="header-section-number">2.2.1.2.1</span> Rating Task</h5>
The first phase of the study (rating task, Fig. <a href="decoy.html#fig:decoy-methods">2.2</a>a) was introduced as a “property rental price guessing game.” The task involved estimating the market rental price of residential real estate by viewing an image of the exterior of a series of properties. On each trial, an image of a property was shown along with a horizontal slider for a maximum of 60 seconds. The task was to guess the market rental price of the property, i.e. the dollar amount that an average person would be willing to pay per month to rent it, and to indicate it on a slider scale. The slider ranged from $0 to $2500 and the initial position of the slider was randomized on each trial. There were a total of 250 unique house images, each presented twice in randomized order (for a total of 500 trials per participant). The 250 houses had been selected to have the lowest average choice variability in a pilot study involving 30 distinct participants and a larger set of properties (<span class="math inline">\(n=450\)</span>), which we conducted before the main experiment. We used participant ratings from the pilot dataset to include/exclude participants. After phase 1, we correlated the 250 ratings for each participant against the average ratings obtained from the pilot study. Participants with a Spearman’s rank correlation of <span class="math inline">\(\rho&lt; 0.7\)</span> were excluded; the rest (<span class="math inline">\(n\)</span> = 231) were invited to progress to the choice task.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-methods"></span>
<img src="figures/decoy-methods.png" alt="Experimental design. $\textbf{A:}$ Participants first played a “property price guessing game”.  On each trial they estimated the monthly rental value (in dollars) of a residential property, using a sliding scale. $\textbf{B:}$ After discarding properties with inconsistent responses, ratings were sorted into deciles for each participant. These bins were used to select stimuli for targets A and B (deciles 3 and 8 of estimated ratings; red and blue squares), and decoy stimuli. Each choice task stimulus was created by matching a property with a given decile estimated value (quality; attribute $j$) to a new rental price (economy; attribute $i$) on a 10x10 grid. Eight property/price combinations were generated for each cell in the grid that lay below the diagonal (white cells), and 2 property/price combinations for each cell above the diagonal (grey cells). $\textbf{C:}$ Participants then played a “best value property hunting game” in which they were asked to rank 3 stimuli according to their economy/quality trade off. Two of the stimuli corresponded to the targets A and B, the third stimulus was the decoy. A star rating system was used as a reminder of their previous price estimation judgment." width="90%" />
<p class="caption">
Figure 2.2: Experimental design. <span class="math inline">\(\textbf{A:}\)</span> Participants first played a “property price guessing game.” On each trial they estimated the monthly rental value (in dollars) of a residential property, using a sliding scale. <span class="math inline">\(\textbf{B:}\)</span> After discarding properties with inconsistent responses, ratings were sorted into deciles for each participant. These bins were used to select stimuli for targets A and B (deciles 3 and 8 of estimated ratings; red and blue squares), and decoy stimuli. Each choice task stimulus was created by matching a property with a given decile estimated value (quality; attribute <span class="math inline">\(j\)</span>) to a new rental price (economy; attribute <span class="math inline">\(i\)</span>) on a 10x10 grid. Eight property/price combinations were generated for each cell in the grid that lay below the diagonal (white cells), and 2 property/price combinations for each cell above the diagonal (grey cells). <span class="math inline">\(\textbf{C:}\)</span> Participants then played a “best value property hunting game” in which they were asked to rank 3 stimuli according to their economy/quality trade off. Two of the stimuli corresponded to the targets A and B, the third stimulus was the decoy. A star rating system was used as a reminder of their previous price estimation judgment.
</p>
</div>
</div>
<div id="choice-task" class="section level5" number="2.2.1.2.2">
<h5><span class="header-section-number">2.2.1.2.2</span> Choice Task</h5>
<p>We introduced phases 2 and 3 as a “best-value property hunting game” (choice task, Fig. <a href="decoy.html#fig:decoy-methods">2.2</a>c). Here, participants were told to imagine that they were a real estate agent recommending to a client the best value house we, a fictitious real estate company, have on offer. On each trial, 3 properties (i.e. choice alternatives) were displayed for a maximum of 60 seconds on left, central and right positions on the screen. Underneath each image we displayed an allocated rental cost (in dollars) and a number of stars. The number of stars was proportional to the value given by the participant in the ratings task, and merely served as a reminder. In piloting, we found that this improved choice consistency. Participants were informed that the property images were a subset of those that they had viewed in phase 1, and that that the stars were related to the ratings they themselves had reported. The task asked participants to press three keyboard buttons (left, down, and right arrow buttons) to indicate their ordered preference from the best-valued house to the worst-valued house. Participants were explicitly instructed that the best-valued house was the one with the highest market value but the lowest allocated rental price. At the end of each block, participants were told how many trials’ recommendations were correct, given their initial ratings. The bonus payment at the end of each phase was proportional to their accuracy.</p>
<p>Unbeknownst to participants, the options were carefully selected for each participant individually to allow us to test our hypotheses of interest (Fig. <a href="decoy.html#fig:decoy-methods">2.2</a>b). First, for each participant we filtered out the 90 properties with the highest rating variability, i.e. the highest absolute deviance between the two ratings. Second, the remaining 160 properties were binned into quality deciles (attribute <span class="math inline">\(i\)</span>) on the basis of each participant’s ratings. The binned properties could then be associated with an allocated rental cost that was drawn uniformly from within the range of dollar values that defined each decile (attribute <span class="math inline">\(j\)</span>). This allowed us to select, on each trial, three stimuli that differed on two dimensions: two targets A and B, and a decoy stimulus. Target A was always a property drawn from the 3rd decile of quality (i.e. participant rating) and the 8th decile of economy (i.e. the 3rd decile of cost); target B was always drawn from the 8th decile of quality and the 3rd decile of economy (i.e. the 8th decile of cost). The decoy stimulus was sampled exhaustively from the full attribute space (any of the 10 quality x 10 economy bins). Thus, targets A and B were equally valued options (A being low quality and low cost, and B being high quality and high cost) and the decoy stimulus could be both superior or inferior in value. Participants completed a total of 530 trials in the second part of the study. The third part constituted an additional 530 trials of the same task.</p>
</div>
</div>
<div id="analyses" class="section level4" number="2.2.1.3">
<h4><span class="header-section-number">2.2.1.3</span> Analyses</h4>
<p>We calculated relative choice share for the “low” item A over the “high” item B by calculating the proportion of trials for which A was chosen before B across all trials in each decoy decile bin. We were able to do this for both inferior and superior decoys, as participants provided us with ranked preference for all three items. Thus, even if on a given trial they chose the decoy, they still reported whether they preferred A over B or vice-versa. This allowed us to plot the relative preference for A over B across the full attribute space (<span class="math inline">\(RCS_{ij}\)</span>).</p>
<div id="conventional-decoy-analyses" class="section level5" number="2.2.1.3.1">
<h5><span class="header-section-number">2.2.1.3.1</span> Conventional Decoy Analyses</h5>
<p>We first adopted a standard approach from previous studies that have focused on estimating the effects of the three classic decoy effects, that is, calculating RCS for <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_c\)</span> and <span class="math inline">\(D_s\)</span>. To this end, we defined portions of the influence map that corresponded to the traditionally defined positions of attraction, compromise and similarity decoys (Fig. <a href="decoy.html#fig:decoy-classic">2.3</a>a). We also included an additional decoy set that we called “repulsion” decoys (<span class="math inline">\(D_r\)</span>): these were mirror-symmetric to the attraction decoys but located in the upper triangle of the influence map where the decoy was the objectively best option (i.e. a set of “superior” decoys). We then calculated the RCS for <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_c\)</span>, <span class="math inline">\(D_s\)</span> and <span class="math inline">\(D_r\)</span>, each defined with respect to target A and target B. The strength of each effect was defined as the difference in RCS for each decoy set defined with respect to targets A and B. We tested each effect against zero for statistical significance via a single sample <span class="math inline">\(t\)</span> test.</p>
</div>
<div id="interrelationship-of-effects" class="section level5" number="2.2.1.3.2">
<h5><span class="header-section-number">2.2.1.3.2</span> Interrelationship of Effects</h5>
<p>To investigate the associations between the three classic decoy effects in our cohort, we calculated Pearson’s correlation coefficients. We estimated those pairwise for each of the three possible combinations of effects (<span class="math inline">\(D_c\)</span>-<span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_c\)</span>-<span class="math inline">\(D_s\)</span>, and <span class="math inline">\(D_a\)</span>-<span class="math inline">\(D_s\)</span>), based on the strength of the effects for each participant in our sample.</p>
</div>
<div id="map-of-decoy-influence" class="section level5" number="2.2.1.3.3">
<h5><span class="header-section-number">2.2.1.3.3</span> Map of Decoy Influence</h5>
<p>Our major goal for this project was to go beyond conventional analyses and chart decoy influence across the attribute space. The ordered RCS for option A over B in each decoy decile bin across all possible attribute combinations constitutes the map of decoy influence. In addition to the decoy map, as a sanity check, we also plotted preferences for the two targets <em>over</em> the decoy item. We did this by calculating the RCS for option A over the decoy D (i.e. in what proportion of trials did the participant choose A before D?), as well as the RCS for option B over the decoy D in each of the 10x10 decoy decile bins.</p>
</div>
<div id="map-decomposition" class="section level5" number="2.2.1.3.4">
<h5><span class="header-section-number">2.2.1.3.4</span> Map Decomposition</h5>
<p>The resulting map of decoy influence allowed us to not only ask how the three classic decoy effects are interrelated in our cohort of participants, but to seek structure beyond these three locations in attribute space. Using an exhaustive range of decoy locations allowed us to use dimensionality reduction approaches to examine the (potentially distinct) factors from which the map of decoy influence is composed. We used singular value decomposition (SVD) to identify factors contributing to the map of preference for A &gt; B and calculate the variance explained by these factors. To this end, we flattened the 10x10 decoy map for each participant into a vector of values and constructed a large matrix of RCS, where each row indexed an individual participant and each column – the decoy location in attribute space. SVD produces a set of basis functions, or factors, and allows us to examine the amount of variance explained by them in the empirical data. This, in turn, illustrates the structure of decoy effects across participants. Knowing this structure can give us clues as to whether distinct mechanisms drive decoy influence across different locations in attribute space, or alternatively, if the general structure of RCS can be explained by a single factor.</p>
</div>
</div>
<div id="data-code-availability-statement" class="section level4" number="2.2.1.4">
<h4><span class="header-section-number">2.2.1.4</span> Data &amp; Code Availability Statement</h4>
<p>All data and code to reproduce the analyses are available in the OSF repository (<a href="https://osf.io/u6br3/">https://osf.io/u6br3/</a>) for this project.</p>
</div>
</div>
<div id="results" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Results</h3>
<div id="conventional-decoy-analyses-1" class="section level4" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> Conventional Decoy Analyses</h4>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-classic"></span>
<img src="figures/decoy-classic.png" alt="Conventional decoy analyses. $\textbf{A:}$ Each panel illustrates the chosen locations in decoy space for compromise ($D_c$), attraction ($D_a$), similarity ($D_s$) and repulsion ($D_r$) decoys (boxes). The blue-yellow color scale illustrates relative preference for target A over B (warmer colors) or vice versa (colder colors) at each location. Black circles indicate the locations of targets A and B.  $\textbf{B:}$ Average choice share for target A as a function of decoy location across $D_c$, $D_a$, $D_s$ and $D_r$. Blue dots are human data, shaded lines are model fit of adaptive gain model (see below). Bars/shaded area signal M±SEM *** indicates p &lt; 0.001. $\textbf{C:}$ Correlations between the attraction, compromise and similarity effects. Each dot is a single participant; the decoy estimate is calculated as the difference between the RCS for given decoy with respect to targets A and B. " width="100%" />
<p class="caption">
Figure 2.3: Conventional decoy analyses. <span class="math inline">\(\textbf{A:}\)</span> Each panel illustrates the chosen locations in decoy space for compromise (<span class="math inline">\(D_c\)</span>), attraction (<span class="math inline">\(D_a\)</span>), similarity (<span class="math inline">\(D_s\)</span>) and repulsion (<span class="math inline">\(D_r\)</span>) decoys (boxes). The blue-yellow color scale illustrates relative preference for target A over B (warmer colors) or vice versa (colder colors) at each location. Black circles indicate the locations of targets A and B. <span class="math inline">\(\textbf{B:}\)</span> Average choice share for target A as a function of decoy location across <span class="math inline">\(D_c\)</span>, <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_s\)</span> and <span class="math inline">\(D_r\)</span>. Blue dots are human data, shaded lines are model fit of adaptive gain model (see below). Bars/shaded area signal M±SEM *** indicates p &lt; 0.001. <span class="math inline">\(\textbf{C:}\)</span> Correlations between the attraction, compromise and similarity effects. Each dot is a single participant; the decoy estimate is calculated as the difference between the RCS for given decoy with respect to targets A and B.
</p>
</div>
<p>Fig. <a href="decoy.html#fig:decoy-classic">2.3</a>b illustrates the difference in choice share driven by decoys in the 4 locations associated with <span class="math inline">\(D_c\)</span>, <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_s\)</span> and <span class="math inline">\(D_r\)</span> (defined in Fig. <a href="decoy.html#fig:decoy-classic">2.3</a>a). As a first general observation, despite the careful sampling of targets that were matched in price/quality ratio according to participants’ responses in the valuation phase, and despite the incentives offered for consistent responding, participants exhibited a bias towards the high item B (average RCS ~ 0.7) over the low item A. Despite this additive bias, however, decoys had a clear and robust influence on choice. Participants exhibited clear attraction (<span class="math inline">\(t_{188}=4.74\)</span>, <span class="math inline">\(p&lt;0.001\)</span>) and compromise (<span class="math inline">\(t_{188}=6.31, p&lt;0.001\)</span>) effects, which were statistically significant and in the expected direction. There was also empirical support for a repulsion effect (<span class="math inline">\(t_{188}=3.45\)</span>, <span class="math inline">\(p&lt;0.001\)</span>) from superior decoys. On average, the presence of attraction, compromise or repulsion decoys shifted preferences from A to B by about 3-5%. However, we failed to replicate the similarity decoy effect (<span class="math inline">\(t_{188}=0.41\)</span>, <span class="math inline">\(p = 0.68\)</span>).</p>
<div id="interrelationship-of-effects-1" class="section level5" number="2.2.2.1.1">
<h5><span class="header-section-number">2.2.2.1.1</span> Interrelationship of Effects</h5>
<p>We observed a positive association between the attraction and compromise effects (<span class="math inline">\(r=0.72\)</span>, <span class="math inline">\(p&lt;0.001\)</span>) and a negative relation between the similarity effect and both compromise (<span class="math inline">\(r=-0.59\)</span>, <span class="math inline">\(p&lt;0.001\)</span>) and attraction (<span class="math inline">\(r=-0.46\)</span>, <span class="math inline">\(p&lt;0.001\)</span>) effects. Note that the latter correlations were observed despite the fact that in our data, the similarity effect was on average non-significant. These are shown in Fig. <a href="decoy.html#fig:decoy-classic">2.3</a>c.</p>
</div>
<div id="map-of-decoy-influence-1" class="section level5" number="2.2.2.1.2">
<h5><span class="header-section-number">2.2.2.1.2</span> Map of Decoy Influence</h5>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-map"></span>
<img src="figures/decoy-map.png" alt="Map of decoy influence. $\textbf{A:}$ Decoy influence map showing $RCS_{ij}$ for A over B (left panels), A over D (middle panels) and B over D (right panels). Upper plots are the human data and lower plots are the same data for the simulated adaptive gain model (see below). The dashed line signals isopreference and the black circles are the targets A and B." width="100%" />
<p class="caption">
Figure 2.4: Map of decoy influence. <span class="math inline">\(\textbf{A:}\)</span> Decoy influence map showing <span class="math inline">\(RCS_{ij}\)</span> for A over B (left panels), A over D (middle panels) and B over D (right panels). Upper plots are the human data and lower plots are the same data for the simulated adaptive gain model (see below). The dashed line signals isopreference and the black circles are the targets A and B.
</p>
</div>
<p>The full map of decoy influence <span class="math inline">\(RCS_{ij}\)</span> is shown in Fig. <a href="decoy.html#fig:decoy-map">2.4</a>a. Visual inspection reveals that the map has rich structure beyond the traditional decoy locations. Relative preferences for A and B seem to be driven by a dynamic of attraction and repulsion that depends on the position of the decoy with respect to each target stimulus. Robust attraction effects (whereby the presence of a decoy that is dominated by A shifts preferences towards A) were mirrored by strong repulsion effects (whereby a decoy that dominates A shifts preferences towards B). Attraction and repulsion were observed for both targets in approximate symmetry.</p>
<p>The map charting preference for A &gt; D (Fig. <a href="decoy.html#fig:decoy-map">2.4</a>b) is in line with the expected pattern, whereby inferior decoys, i.e. those under the left diagonal, or line of theoretical isopreference, are chosen less often (warmer colors) than superior decoys (colder colors). Similarly, the map charting preference for B &gt; D (Fig. <a href="decoy.html#fig:decoy-map">2.4</a>c) adheres to this general pattern. In theory, if A and B were equally preferred, these two maps would be identical. However, in line with the observed preference bias for the high value item B, we see that overall, preferences for A &gt; D appear weaker compared to preferences for B &gt; D.</p>
</div>
<div id="map-decomposition-1" class="section level5" number="2.2.2.1.3">
<h5><span class="header-section-number">2.2.2.1.3</span> Map Decomposition</h5>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-svd"></span>
<img src="figures/decoy-svd.png" alt="Map decomposition. $\textbf{A:}$ First five components obtained from singular value decomposition (SVD) of the RCS for A vs. B. Upper plots are the human data and lower plots are the model. $\textbf{B:}$ Correlation between component scores for components 1-3 between the human and the best-fitting model. For components 1-3, this correlation was very high. $\textbf{C:}$ The variance explained by each component obtained by SVD for the humans (left panel) and the model (right panel). Note that the y-axis is on a log scale; the data is dominated by the first component in both cases." width="100%" />
<p class="caption">
Figure 2.5: Map decomposition. <span class="math inline">\(\textbf{A:}\)</span> First five components obtained from singular value decomposition (SVD) of the RCS for A vs. B. Upper plots are the human data and lower plots are the model. <span class="math inline">\(\textbf{B:}\)</span> Correlation between component scores for components 1-3 between the human and the best-fitting model. For components 1-3, this correlation was very high. <span class="math inline">\(\textbf{C:}\)</span> The variance explained by each component obtained by SVD for the humans (left panel) and the model (right panel). Note that the y-axis is on a log scale; the data is dominated by the first component in both cases.
</p>
</div>
<p>Next, we sought to examine the structure of the map of decoy influence (Fig. <a href="decoy.html#fig:decoy-map">2.4</a>a) via dimensionality reduction. The first 5 factors identified by SVD are visualized in Fig. <a href="decoy.html#fig:decoy-svd">2.5</a>a (top row). The first factor accounted for 95% of the variance in the data, suggesting that there is a single explanatory variable that drives decoy effects across participants (Fig. <a href="decoy.html#fig:decoy-svd">2.5</a>c, right panel, note the logarithmic scale on the y-axis). This finding is consistent with the correlation results reported above, showing the robust interrelationships between the three classic effects in our participants.</p>
</div>
</div>
</div>
<div id="interim-discussion" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Interim Discussion</h3>
<p>Conventional decoy analyses of our data replicated the previously extensively documented compromise and attraction effects <span class="citation">(<a href="#ref-huber1982" role="doc-biblioref">J. Huber, Payne, and Puto 1982</a>; <a href="#ref-simonson1989" role="doc-biblioref">Simonson 1989</a>)</span>. However, we failed to replicate the similarity decoy effect in this dataset, consistent with weak or absent similarity effect from other studies <span class="citation">(<a href="#ref-noguchi2014" role="doc-biblioref">Noguchi and Stewart 2014</a>; <a href="#ref-berkowitsch2014" role="doc-biblioref">Berkowitsch, Scheibehenne, and Rieskamp 2014</a>)</span>. Indeed, while the attraction effect tends to be highly robust and consistent across participants, the compromise effect and similarity effects tend to be more idiosyncratic, with a high proportion of participants showing effects which are inverted with respect to the canonical form. For example, in previous studies only a minority of participants show all three effects (numerically) in the expected direction <span class="citation">(e.g. only 23% in <a href="#ref-trueblood2015" role="doc-biblioref">Trueblood, Brown, and Heathcote 2015</a>, we find a comparable figure of 22%)</span>.</p>
<p>Despite the fact that the similarity effect was on average non-significant in our dataset, its strength inversely correlated with the strength of the compromise and attraction effects. Those two effects were in turn positively associated with one another. This pattern of interrelationships of the decoy effects mirrors previous reports <span class="citation">(<a href="#ref-noguchi2014" role="doc-biblioref">Noguchi and Stewart 2014</a>)</span> and is further supported by our results on the structure of the full map of decoy influence. In particular, we found that a single explanatory factor is sufficient to capture 95% of the variability of decoy influence in our dataset.</p>
<p>One interpretation of this data is that the attraction, compromise and similarity effects are not distinct phenomena, but rather they are all driven by a single common computational principle. This principle ensures that decision values of stimuli A and B are repulsed away from proximal decoys on a line which lies perpendicular to the line of isopreference, giving rise firstly to what is traditionally known as the “attraction” effect (a potentially confusing nomenclature, given that decision values for a target are repulsed away from those of the decoy) and secondly to its converse identified here for superior decoys, which we have dubbed the “repulsion” effect. The compromise and similarity effects can arise because these repulsive effects may vary in strength for targets A and B, and/or for inferior and superior decoys, such that repulsive effects “spill over” asymmetrically towards the extremes of the isopreference line (compromise effect) or into the center of the bivariate decoy map (similarity effect). It is the fact that compromise and similarity effects are really driven by asymmetric attraction and repulsion that gives rise to the strong correlations in effects across the cohort observed in this paper and previously <span class="citation">(<a href="#ref-berkowitsch2014" role="doc-biblioref">Berkowitsch, Scheibehenne, and Rieskamp 2014</a>)</span>.</p>
</div>
</div>
<div id="computational-modeling" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Computational Modeling</h2>
<p>These findings suggest that it should be possible to identify a simple model that can reproduce the full decoy influence map with parameters that control the putative repulsive principle, as well as additional degrees of freedom that allow for asymmetric scaling of the attribute values (quality and economy). Previous simulation work demonstrated that a gain-control process can provide a unifying explanation for similar choice biases <span class="citation">(<a href="#ref-li2018" role="doc-biblioref">V. Li et al. 2018</a>)</span>. In this theoretic account, the gain with which alternatives are encoded is adaptively adjusted according to the context provided by other available options. Recent proposals of models for decoy phenomena have also appealed to a similar adaptive framework, whereby decision signals are normalized by context <span class="citation">(<a href="#ref-rigoli2017" role="doc-biblioref">Rigoli et al. 2017</a>; <a href="#ref-daviet2018" role="doc-biblioref">Daviet 2018</a>)</span>.</p>
<div id="adaptive-gain-model" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Adaptive Gain Model</h3>
<p>The normalization principle in the adaptive gain model assumes that for each attribute (e.g. quality or economy), a <em>relative</em> value estimate is computed that reflects the distance between each item and the mean of the available choice set. We thus begin by defining <span class="math inline">\(r(A)\)</span>, which is the value of target A relative to other items, including the decoy, on a given attribute <span class="math inline">\(i\)</span>:
<span class="math display" id="eq:decoy-ag-r">\[\begin{equation}
r(A_i) = v(A_i) - v(avg_i^{ABD})
\tag{2.1}
\end{equation}\]</span>
where <span class="math inline">\(v(avg_i^{ABD})\)</span> corresponds to the average value of the three alternatives on attribute <span class="math inline">\(i\)</span>:
<span class="math display">\[\begin{equation}
v(avg_i^{ABD})=\frac{v(A_i)+v(B_i)+v(D_i)}{3}
\end{equation}\]</span>
The model proposes that the subjective utility <span class="math inline">\(u(A_i)\)</span> of target A on attribute <span class="math inline">\(i\)</span> is computed through a logistic transformation of the relative value, as follows:
<span class="math display" id="eq:decoy-ag">\[\begin{equation}
u_i(A_i) = \frac{1}{1+e^{-(r(A_i)-c_i)\cdot s^{-1}}}
\tag{2.2}
\end{equation}\]</span>
Thus, for each attribute, the utility of each target is given by a logistic function with slope <span class="math inline">\(s\)</span> whose inflection point is the mean value of all items plus an additive bias term <span class="math inline">\(c\)</span>. The additive bias <span class="math inline">\(c\)</span> can potentially vary across attributes.</p>
The utility of target A is a weighed sum of its attributes <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, and the final decision is made by passing the utilities of all three rival stimuli through a softmax function to make a ternary choice:
<span class="math display" id="eq:decoy-item-u">\[\begin{equation}
u(A) = w \cdot u_i(A_i) + (1-w) \cdot u_j(A_j)
\tag{2.3}
\end{equation}\]</span>
<span class="math display" id="eq:decoy-prob">\[\begin{equation}
p(A) = \frac{e^{\tau u(A)}}{e^{\tau u(A)}+e^{\tau u(B)}+e^{\tau u(D)}}
\tag{2.4}
\end{equation}\]</span>
In addition to the softmax temperature <span class="math inline">\(\tau\)</span>, the model potentially has 4 free parameters of interest: the slope <span class="math inline">\(s\)</span>, and inflection points <span class="math inline">\(c_i\)</span> and <span class="math inline">\(c_j\)</span> of the logistic function Eq. <a href="decoy.html#eq:decoy-item-u">(2.3)</a>, and the weighing parameter <span class="math inline">\(w\)</span> in Eq. <a href="decoy.html#eq:decoy-prob">(2.4)</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-sim"></span>
<img src="figures/decoy-sim.png" alt="Simulated maps of decoy influence. $\textbf{A:}$ Effect of varying the parameter $w$ from low (left panels) to high (right panels). This parameter controls the relative preference for low price/quality to high price/quality items. $\textbf{B:}$ The effect of varying the parameter $s$ from high to low. This parameter controls whether A and B are equally preferred, or whether there is decoy-like distortion. $\textbf{C:}$  The effect of varying the difference of bias terms $c_i$=$-c_j$ from negative (left panels) to positive (right panels). Varying this difference alters whether the maximal distortion occurs proximal to target A (left) or target B (right). $\textbf{D:}$ The effect of varying the sum of bias terms $c_i=c_j$ from negative (left panels) to positive (right panels). Varying this difference alters whether the maximal distortion occurs for inferior decoys (left) or superior decoys (right). Red arrows highlight directions of repulsion, with arrow width schematically representing the strength of the effect. " width="100%" />
<p class="caption">
Figure 2.6: Simulated maps of decoy influence. <span class="math inline">\(\textbf{A:}\)</span> Effect of varying the parameter <span class="math inline">\(w\)</span> from low (left panels) to high (right panels). This parameter controls the relative preference for low price/quality to high price/quality items. <span class="math inline">\(\textbf{B:}\)</span> The effect of varying the parameter <span class="math inline">\(s\)</span> from high to low. This parameter controls whether A and B are equally preferred, or whether there is decoy-like distortion. <span class="math inline">\(\textbf{C:}\)</span> The effect of varying the difference of bias terms <span class="math inline">\(c_i\)</span>=<span class="math inline">\(-c_j\)</span> from negative (left panels) to positive (right panels). Varying this difference alters whether the maximal distortion occurs proximal to target A (left) or target B (right). <span class="math inline">\(\textbf{D:}\)</span> The effect of varying the sum of bias terms <span class="math inline">\(c_i=c_j\)</span> from negative (left panels) to positive (right panels). Varying this difference alters whether the maximal distortion occurs for inferior decoys (left) or superior decoys (right). Red arrows highlight directions of repulsion, with arrow width schematically representing the strength of the effect.
</p>
</div>
<div id="model-simulations" class="section level4" number="2.3.1.1">
<h4><span class="header-section-number">2.3.1.1</span> Model Simulations</h4>
<p>We explore the effects of manipulating these parameters on the predicted decoy influence map in Fig. <a href="decoy.html#fig:decoy-sim">2.6</a>; a full description of the parameters used is available in Table <a href="#tab:decoy-table"><strong>??</strong></a>. This figure shows how the model can systematically account for not only the pattern observed in the current experiment, but also those from previous (and potentially contradictory) reports. In Fig. <a href="decoy.html#fig:decoy-sim">2.6</a>a we show the effect of manipulating the parameter <span class="math inline">\(w\)</span>. This simply shows how we can tip the balance of preferring A over B according to the relative weight given to each attribute.</p>
<p>In Fig. <a href="decoy.html#fig:decoy-sim">2.6</a>b, with <span class="math inline">\(w\)</span> now fixed to 0.5 (equal weighing of the two attributes), we show how the decoy effects grow in strength with <span class="math inline">\(s\)</span>. Above each plot the relative positive or negative strength of the compromise (<span class="math inline">\(D_c\)</span>), attraction (<span class="math inline">\(D_a\)</span>), similarity (<span class="math inline">\(D_s\)</span>) and repulsion (<span class="math inline">\(D_r\)</span>) effects are shown in a bar plot. As can be seen, the attraction and repulsion effects grow as <span class="math inline">\(s\)</span> grows, including a weak compromise effect but no similarity effect.</p>
<p>Fig. <a href="decoy.html#fig:decoy-sim">2.6</a>c shows the influence of varying <span class="math inline">\(c_i=-(c_j)\)</span> while <span class="math inline">\(s\)</span> and <span class="math inline">\(w\)</span> are fixed. This has the effect of shifting the relative strength of the attraction/repulsion effect for targets A and B. For example, when <span class="math inline">\(c_i&gt;c_j\)</span> the attraction/repulsion effects are strongest for the target A, whereas when <span class="math inline">\(c_j&gt;c_i\)</span> they are strongest for B (red arrows). However, because these effects cancel out symmetrically, this does not affect the overall RCS for <span class="math inline">\(D_c\)</span>, <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_s\)</span> or <span class="math inline">\(D_r\)</span>.</p>
Finally, varying <span class="math inline">\(c_i=c_j\)</span> (Fig. <a href="decoy.html#fig:decoy-sim">2.6</a>d) brings about an asymmetric distortion whereby either attraction effects are stronger (i.e. below the isopreference line) or repulsion effects are stronger (in the superior decoy portion of space). In addition to varying the relative strength of attraction and repulsion, this allows the compromise effect to vary from positive to negative, as described in previous studies; it allows a weak similarity effect to emerge. Combinations of all of these factors give the model systematic flexibility to account for a wide range of observed effects.
<p>Interestingly, the simulations shown in Fig. <a href="decoy.html#fig:decoy-sim">2.6</a>d allow us to make a new prediction about the human data. As seen in the bar plots accompanying each predicted influence map, when <span class="math inline">\(c_i\)</span> and <span class="math inline">\(c_j\)</span> are both negative, the compromise effect is positive and attraction is stronger than repulsion. By contrast, when <span class="math inline">\(c_i\)</span> and <span class="math inline">\(c_j\)</span> are both positive, the compromise effect is negative and the repulsion effect is stronger than attraction. The model thus predicts that on average, in the human data there will be a correlation between the (signed) compromise effect and the relative magnitude of attraction vs repulsion. This is plotted in Fig. <a href="decoy.html#fig:decoy-new-rel">2.7</a>, and as can be seen this prediction holds for the data we collected (<span class="math inline">\(r=0.57\)</span>, <span class="math inline">\(p&lt;0.001\)</span>). Of note, this effect was driven both by a positive correlation between the compromise effect and the strength of repulsion (<span class="math inline">\(r=0.43\)</span>, <span class="math inline">\(p&lt;0.001\)</span>), as well as the correlation between compromise and attraction effects described above.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-new-rel"></span>
<img src="figures/decoy-new-rel.png" alt="Correlation between the compromise effect and the relative strength of attraction vs repulsion in the human data.  Each dot is a participant; the blue line is the best fitting linear trend." width="50%" />
<p class="caption">
Figure 2.7: Correlation between the compromise effect and the relative strength of attraction vs repulsion in the human data. Each dot is a participant; the blue line is the best fitting linear trend.
</p>
</div>
</div>
<div id="model-fitting" class="section level4" number="2.3.1.2">
<h4><span class="header-section-number">2.3.1.2</span> Model Fitting</h4>
<p>Next, we estimated the model parameters which best fit the data for each participant in our experiment to assess whether the model was able to capture the results at the individual as well as aggregate level.</p>
<div id="model-fitting-methods" class="section level5" number="2.3.1.2.1">
<h5><span class="header-section-number">2.3.1.2.1</span> Model Fitting Methods</h5>
<p>The model provided predictions about probabilities of choosing option A over B, which allowed us to compute model likelihoods on each trial. Those likelihoods were then used for model fitting and comparison. We estimated the model for each participant individually using gradient descent with the <code>globalsearch</code> function from the MATLAB Optimization Toolbox.</p>
</div>
<div id="model-fitting-results" class="section level5" number="2.3.1.2.2">
<h5><span class="header-section-number">2.3.1.2.2</span> Model Fitting Results</h5>
<p>Fitting this 5-parameter (<span class="math inline">\(\tau\)</span>, <span class="math inline">\(s\)</span>, <span class="math inline">\(c_i\)</span>, <span class="math inline">\(c_j\)</span>, <span class="math inline">\(w\)</span>) model to human data, we can fully recreate the decoy effects observed in this study using both conventional (Fig. <a href="decoy.html#fig:decoy-classic">2.3</a>b) and novel (Fig. <a href="decoy.html#fig:decoy-map">2.4</a>d-f) analysis methods. Specifically, the model captured almost exactly the pattern of traditional decoy effects, in terms of the relative impact on RCS of <span class="math inline">\(D_a\)</span>, <span class="math inline">\(D_c\)</span> and <span class="math inline">\(D_s\)</span>, as well as the repulsion decoy <span class="math inline">\(D_r\)</span> (blue shaded lines in Fig. <a href="decoy.html#fig:decoy-classic">2.3</a>b). The model reproduced the pattern of preferences for target A &gt; target B qualitatively and quantitatively across the decoy space, and when we applied SVD to the model data generated under the best-fitting parametrization for each participant, the first five components that emerged were nearly identical to those for humans, and the first model component explaining 97% of the variance (Fig. <a href="decoy.html#fig:decoy-svd">2.5</a>c). When we plotted the estimated singular values for the first three components for humans and the best-fitting model, we found them to be very tightly correlated (Fig. <a href="decoy.html#fig:decoy-svd">2.5</a>b). The model also displayed the same pattern of positive association between attraction and compromise effect (<span class="math inline">\(r=0.86\)</span>) and negative association between the similarity and attraction (<span class="math inline">\(r=-0.85\)</span>) and similarity and compromise effects (<span class="math inline">\(r=-0.94\)</span>). In other words, the model captures the human data very closely, both at the individual and the aggregate level.</p>
</div>
</div>
</div>
<div id="model-comparisons" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Model Comparisons</h3>
<p>The adaptive gain model belongs to a class of models which describe contextual biases as arising from normalization among populations of neurons. Normalization models share commitment to the idea that information is divisively normalized by the local context. The precise mathematical formulation of this procedure, however, differs between theoretical accounts. Some models, for instance, use measures of the central tendency of the features present in the local context <span class="citation">(e.g. the average of attribute values, <a href="#ref-louie2013" role="doc-biblioref">Louie, Khaw, and Glimcher 2013</a>)</span>, while other appeal to measures of the dispersion of features <span class="citation">(e.g. the range of attribute values, <a href="#ref-soltani2012" role="doc-biblioref">Soltani, De Martino, and Camerer 2012</a>)</span>. Similarly, models might encode features linearly <span class="citation">(e.g. <a href="#ref-bushong2021" role="doc-biblioref">Bushong, Rabin, and Schwartzstein 2021</a>)</span> or compress them prior to normalization <span class="citation">(e.g. via a logistic operation as in the model described here and in <a href="#ref-rigoli2019" role="doc-biblioref">Rigoli 2019</a>)</span>. Below we highlight several prominent normalization models in the literature. Our aim was to compare our model to a broad space of alternative accounts that similarly assume the value of each target (on each attribute) is encoded relative to its competitors. Thus, we began our model comparison exercise with contrasts with those well established competitors.</p>
<p>The classic formulation of the divisive normalization theoretical tradition (henceforth vanilla divisive normalization, VDN) can be broadly encapsulated in the following expression <span class="citation">(<a href="#ref-carandini2012" role="doc-biblioref">Carandini and Heeger 2012</a>; <a href="#ref-louie2013" role="doc-biblioref">Louie, Khaw, and Glimcher 2013</a>)</span>:
<span class="math display" id="eq:decoy-vdn">\[\begin{equation}
u_i(A_i)^{VDN} = \frac{v(A_i)}{v(avg_i^{ABD})+c_i}
\tag{2.5}
\end{equation}\]</span>
where <span class="math inline">\(c_i\)</span> is a small regularization constant. The model assumes that decision inputs are divided by the current contextual expectation, that is, the average of all current options.</p>
<p>In one variant of this model, which has been recently and successfully used to account for decoy effects, this normalization is also “recurrent,” i.e. it overweighs the focal item’s contribution to normalization <span class="citation">(<a href="#ref-daviet2018" role="doc-biblioref">Daviet 2018</a>; <a href="#ref-webb2021" role="doc-biblioref">Webb, Glimcher, and Louie 2021</a>)</span>:
<span class="math display" id="eq:decoy-rdn">\[\begin{equation}
u_i(A_i)^{RDN} = \frac{v(A_i)}{v(A_i)+v(avg_i^{ABD})+c_i}
\tag{2.6}
\end{equation}\]</span></p>
<p>Note that the adaptive gain model (Eq. <a href="decoy.html#eq:decoy-ag">(2.2)</a>) is equivalent to a form of the recurrent divisive normalization model in which the values are exponentiated prior to normalization:
<span class="math display" id="eq:decoy-rdn">\[\begin{equation}
u_i(A_i)^{AG} = \frac{\exp(\frac{v(A_i)}{s})}{\exp(\frac{v(A_i)}{s})+\exp(\frac{v(avg_i^{ABD})+c_i}{s})}
\tag{2.6}
\end{equation}\]</span></p>
<p>Alongside these two models, we also considered a class of contextual normalization, which uses the range (rather than the average) of attribute values being encoded on a given trial to normalize an imperative stimulus:
<span class="math display" id="eq:decoy-rn">\[\begin{equation}
u_i(A_i)^{RN} = \beta \frac{v(A_i)}{v(rng_i^{ABD})}
\tag{2.7}
\end{equation}\]</span>
where <span class="math inline">\(v(rng_i^{ABD})\)</span> corresponds to the corresponds to the difference between highest and lowest values of attribute <span class="math inline">\(i\)</span> across all stimuli (target or decoy) in the trial and <span class="math inline">\(\beta\)</span> is a scaling term <span class="citation">(<a href="#ref-soltani2012" role="doc-biblioref">Soltani, De Martino, and Camerer 2012</a>; <a href="#ref-bushong2021" role="doc-biblioref">Bushong, Rabin, and Schwartzstein 2021</a>)</span>.</p>
<div id="methods-1" class="section level4" number="2.3.2.1">
<h4><span class="header-section-number">2.3.2.1</span> Methods</h4>
<p>We compared the fit of each of the candidate normalization accounts to adaptive gain in a direct model comparison exercise. To achieve this, we used Bayesian model selection on cross-validated model evidence. Cross-validation involved estimating model parameters from one half of trials (by comparing fits to preferences between target items A and B, as well as preferences between target and decoys) and computing log-likelihoods from the held-out trials. We implemented this using the <code>globalsearch</code> function from the MATLAB Optimization Toolbox. For Bayesian model selection, we used the Statistical Parametric Mapping Toolbox <span class="citation">(<a href="#ref-stephan2009" role="doc-biblioref">Stephan et al. 2009</a>)</span>.</p>
</div>
<div id="results-1" class="section level4" number="2.3.2.2">
<h4><span class="header-section-number">2.3.2.2</span> Results</h4>
<p>Comparisons between cross-validated model likelihoods revealed that the exceedance probability for the adaptive gain model over each of the three competitor models (vanilla divisive normalization, recurrent divisive normalization and range normalization) was 0.99, providing decisive evidence for the former over each of the latter.</p>
</div>
</div>
<div id="charting-normalization-models-of-decoy-influence" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Charting Normalization Models of Decoy Influence</h3>
<div id="grandmother-model" class="section level4" number="2.3.3.1">
<h4><span class="header-section-number">2.3.3.1</span> Grandmother Model</h4>
<p>To go beyond those established models and compare our model to a broader range of competitors, we also devised and fit a more flexible model which encompassed a large space of possible normalization schemes. This “grandmother” model could capture the encoding scheme proposed by the 4 models introduced above, along with a number of other “hybrid” models. Thus, exploring the parameter space of the grandmother model allows us to explore the space of putative normalization schemes, including those that are yet to be described in the literature, and chart how different model features translate to predicted patterns of decoy influence.</p>
<p>The model had the general form:</p>
<p><span class="math display" id="eq:grandmother">\[\begin{equation}
u_i(A_i) = \frac{1}{\beta_1 + e^{-(v(A_i)^k - \mu_i^k)(s \cdot k)^{-1}}}
\tag{2.8}
\end{equation}\]</span>
where
<span class="math display">\[\begin{equation}
\mu_i = c_i+ \beta_2  \cdot v(avg^{ABD}_i)+(1-\beta_2)\cdot v(rng^{ABD}_i)
\end{equation}\]</span></p>
<p>We focus on free parameters <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(k\)</span>, which respectively encode the tendency to engage in asymmetric weighing of the evaluated (recurrent) item (<span class="math inline">\(\beta_1\)</span>), the tendency to normalize with respect to the mean vs range (<span class="math inline">\(\beta_2\)</span>) and the extent to which inputs are compressed before being transduced (<span class="math inline">\(k\)</span>).</p>
<p>Parameter <span class="math inline">\(k\)</span> facilitated comparison between the adaptive gain model (which incorporates a logistic transformation) and other normalization models (which do not), by leveraging the following limit:</p>
<p><span class="math display" id="eq:decoy-limit">\[\begin{equation}
\lim_{k \to 0} \frac{x^k-1}{k} = \log_e x, \forall x&gt;0 
\tag{2.9}
\end{equation}\]</span></p>
<p>Thus, substituting <span class="math inline">\(x\)</span> above for our variables of interest, <span class="math inline">\(v(A_i)\)</span> and <span class="math inline">\(\mu\)</span>, we obtain:</p>
<p><span class="math display" id="eq:decoy-limit-eq">\[\begin{equation}
\log v(A_i) - \log \mu_i = \lim_{k \to 0} \frac{v(A_i)^k-1}{k}-\frac{\mu_i^k-1}{k} = \lim_{k \to 0} \frac{v(A_i)^k - \mu_i^k}{k}
\tag{2.10}
\end{equation}\]</span></p>
<p>Thus, <span class="math inline">\(k\)</span> thus corresponds to the extent to which inputs are compressed before being transduced, with <span class="math inline">\(k\sim 0\)</span> signifying a logarithmic compression, and <span class="math inline">\(k=1\)</span> signifying linear input. Simulations revealed that implementing a value of <span class="math inline">\(k\)</span> as high as 0.001 satisfactorily approximates the natural logarithm (Fig. <a href="decoy.html#fig:decoy-log">2.8</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-log"></span>
<img src="figures/decoy-log.png" alt="Illustration of variations in parameter $k$. $\textbf{A:}$ Using parameter $k$ to interpolate between a linear expression and its logarithmic compression. Note that the darkest green curve ($k=0.001$) lies on the dotted line (logarithmic expression), suggesting that implementing a value of $k$ as high as 0.001 satisfactorily approximates the natural logarithm. $\textbf{B:}$ Exponentiating the function from panel A,  as in the logistic formulation of the grandmother model. Note that the darkest green curve ($k=0.001$) lies on the dotted line (linear expression, $\exp(\log(x)-\log(\mu))=\frac{x}{\mu}$). This demonstrates that we can recover both normalization models that rely on logistic compression and normalization models that do not." width="100%" />
<p class="caption">
Figure 2.8: Illustration of variations in parameter <span class="math inline">\(k\)</span>. <span class="math inline">\(\textbf{A:}\)</span> Using parameter <span class="math inline">\(k\)</span> to interpolate between a linear expression and its logarithmic compression. Note that the darkest green curve (<span class="math inline">\(k=0.001\)</span>) lies on the dotted line (logarithmic expression), suggesting that implementing a value of <span class="math inline">\(k\)</span> as high as 0.001 satisfactorily approximates the natural logarithm. <span class="math inline">\(\textbf{B:}\)</span> Exponentiating the function from panel A, as in the logistic formulation of the grandmother model. Note that the darkest green curve (<span class="math inline">\(k=0.001\)</span>) lies on the dotted line (linear expression, <span class="math inline">\(\exp(\log(x)-\log(\mu))=\frac{x}{\mu}\)</span>). This demonstrates that we can recover both normalization models that rely on logistic compression and normalization models that do not.
</p>
</div>
</div>
<div id="grandmother-model-derivations" class="section level4" number="2.3.3.2">
<h4><span class="header-section-number">2.3.3.2</span> Grandmother Model Derivations</h4>
<p>The adaptive gain, vanilla and recurrent divisive normalization, and range normalization models were nested within the grandmother model. This section shows how each of them can be derived from the general grandmother model equation.</p>
<p>Leveraging the limit from Eq. <a href="decoy.html#eq:decoy-limit">(2.9)</a>, when we plug in the parameter values for vanilla divisive normalization specified in Table <a href="#tab:decoy-table-grandmother"><strong>??</strong></a> into the general grandmother model, we arrive at the formulation of vanilla divisive normalization (as per Eq. <a href="decoy.html#eq:decoy-vdn">(2.5)</a>):</p>
<p><span class="math display">\[\begin{equation}
u_i(A_i) = \frac{1}{e^{-(v(A_i)^{0.001}-\mu_i^{0.001})(0.001)^{-1}}} = \frac{1}{e^{-(\log{v(A_i)}-\log{\mu_i})}} = \frac{1}{\frac{\mu_i}{v(A_i)}} = {\frac{v(A_i)}{\mu_i}} = {\frac{v(A_i)}{v(avg^{ABD}_i)+c_i}}
\end{equation}\]</span></p>
<p>Similarly, we may simplify the grandmother model into recurrent divisive normalization (Eq. <a href="decoy.html#eq:decoy-rdn">(2.6)</a>) by plugging in the relevant parameter values specified in Table <a href="#tab:decoy-table-grandmother"><strong>??</strong></a>:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
u_i(A_i) = \frac{1}{1+e^{-(v(A_i)^{0.001}-\mu_i^{0.001})(0.001)^{-1}}} = \frac{1}{1+e^{-(\log{v(A_i)}-\log{\mu_i})}} = \\
\frac{1}{1+\frac{\mu_i}{v(A_i)}} = {\frac{v(A_i)}{v(A_i)+\mu_i}} = {\frac{v(A_i)}{v(A_i)+v(avg^{ABD}_i)+c_i}}
\end{aligned}
\end{equation}\]</span></p>
<p>Note that if we allow parameter <span class="math inline">\(s\)</span> to vary freely, it assumes the role of a power transform for inputs:</p>
<p><span class="math display" id="eq:decoy-power-eq">\[\begin{equation}
\begin{aligned}
u_i(A_i) = \frac{1}{1+e^{-(v(A_i)^{0.001}-\mu_i^{0.001})(s \cdot 0.001)^{-1}}} = \frac{1}{1+e^{-(\log{v(A_i)}-\log{\mu_i})^{s^{-1}}}} = \\ \frac{1}{1+\frac{\mu_i^{s^{-1}}}{v(A_i)^{s^{-1}}}} =  {\frac{v(A_i)^{s^{-1}}}{v(A_i)^{s^{-1}}+\mu_i^{s^{-1}}}} = {\frac{v(A_i)^{s^{-1}}}{v(A_i)^{s^{-1}}+(v(avg^{ABD}_i)+c_i)^{s^{-1}}}}
\tag{2.11}
\end{aligned}
\end{equation}\]</span></p>
<p>Similarly, we may obtain the formulation of range normalization (Eq. <a href="decoy.html#eq:decoy-rn">(2.7)</a>) by plugging in the relevant parameter values from Table <a href="#tab:decoy-table-grandmother"><strong>??</strong></a> into the general grandmother model:</p>
<p><span class="math display">\[\begin{equation}
u_i(A_i) = \frac{1}{e^{-(v(A_i)^{0.001}-\mu^{0.001}_i)(0.001)^{-1}}} = \frac{1}{e^{-(\log{v(A_i)}-\log{\mu_i})}} = \frac{1}{\frac{\mu_i}{v(A_i)}} = {\frac{v(A_i)}{\mu_i}} = {\frac{v(A_i)}{v(rng^{ABD}_i)}}
\end{equation}\]</span></p>
<p>Finally, we may reduce the grandmother model to adaptive gain (Eq. <a href="decoy.html#eq:decoy-ag">(2.2)</a>) by plugging in the relevant parameter values from Table <a href="#tab:decoy-table-grandmother"><strong>??</strong></a>:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
u_i(A_i) = \frac{1}{1+e^{-(v(A_i)-\mu_i) \cdot s^{-1}}} = \frac{1}{1+e^{-({v(A_i)}-{\mu_i}) \cdot s^{-1}}} = \frac{1}{1+e^{-({v(A_i)}-v(avg^{ABD}_i)-c_i)\cdot s^{-1}}}
\end{aligned}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-grandmother"></span>
<img src="figures/decoy-grandmother.png" alt="Charting decoy models. $\textbf{A:}$ Embedding space for normalization models of decoy effects. A $t$-distributed stochastic neighbour visualization of the maps of decoy influence produced by different variants of the grandmother model. Each map represents a variant of the grandmother model positioned in 2D space such that models with similar decoy influence patterns are nearby, while models with more different decoy patterns are further apart. Heat maps illustrate decoy influence. The rectangle on the left shows a zoomed in version of the denoted subset of embedding space. $\textbf{B-D:}$ These plots depict the zoomed in subset of embedding space presented above. Each model-produced decoy map is denoted as a dot and color coded to indicate parameter value: $\beta_1$ (panel B), $\beta_2$ (panel C), or $k$ (panel D). Human data is represented with a cross." width="100%" />
<p class="caption">
Figure 2.9: Charting decoy models. <span class="math inline">\(\textbf{A:}\)</span> Embedding space for normalization models of decoy effects. A <span class="math inline">\(t\)</span>-distributed stochastic neighbour visualization of the maps of decoy influence produced by different variants of the grandmother model. Each map represents a variant of the grandmother model positioned in 2D space such that models with similar decoy influence patterns are nearby, while models with more different decoy patterns are further apart. Heat maps illustrate decoy influence. The rectangle on the left shows a zoomed in version of the denoted subset of embedding space. <span class="math inline">\(\textbf{B-D:}\)</span> These plots depict the zoomed in subset of embedding space presented above. Each model-produced decoy map is denoted as a dot and color coded to indicate parameter value: <span class="math inline">\(\beta_1\)</span> (panel B), <span class="math inline">\(\beta_2\)</span> (panel C), or <span class="math inline">\(k\)</span> (panel D). Human data is represented with a cross.
</p>
</div>
<p>Note that along with those specific models, which have been described in the literature, there exist many more parametrizations of the grandmother model which correspond to “hybrid” normalization schemes, combining specific features of the above described accounts.</p>
</div>
<div id="fitting-the-grandmother-model" class="section level4" number="2.3.3.3">
<h4><span class="header-section-number">2.3.3.3</span> Fitting the Grandmother Model</h4>
<div id="methods-2" class="section level5" number="2.3.3.3.1">
<h5><span class="header-section-number">2.3.3.3.1</span> Methods</h5>
<p>We fit 125 variations of the grandmother model, by varying <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> in 5 steps between 0 and 1, and <span class="math inline">\(k\)</span> in 5 steps between 0.001 and 1, in addition to the four constrained parametrizations of the grandmother model that result in the vanilla divisive normalization, recurrent divisive normalization, adaptive gain and range normalization models (Table <a href="#tab:decoy-table-grandmother"><strong>??</strong></a>). We used the <span class="math inline">\(t\)</span>-distributed stochastic neighbor embedding (t-SNE) visualization technique <span class="citation">(<a href="#ref-vandermaaten2008" role="doc-biblioref">Van Der Maaten and Hinton 2008</a>)</span> to calculate relations among the resulting decoy influence maps from the 129 parametrizations. We specified the t-SNE hyperparameter perplexity, which controls the number of expected close neighbors, following guidelines in the literature to balance the trade-off between perplexity and the Kullback–Leibler divergence <span class="citation">(<a href="#ref-cao2017" role="doc-biblioref">Cao and Wang 2017</a>)</span>.</p>
</div>
<div id="results-2" class="section level5" number="2.3.3.3.2">
<h5><span class="header-section-number">2.3.3.3.2</span> Results</h5>
<p>Fitting the grandmother model to human data revealed the different patterns of decoy influence predicted by variations of the general coding scheme. The embedding plot in Fig. <a href="decoy.html#fig:decoy-grandmother">2.9</a>a shows the simulated decoy maps for each of the 129 parametrizations of the grandmother model. Neighboring maps reflect models that produce relatively similar patterns of decoy influence (and vice versa for distant points). In Fig. <a href="decoy.html#fig:decoy-grandmother">2.9</a>b-d the points are colored according to levels of <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(k\)</span>, revealing the human data is neighbored by maps generated by models with high values of parameters <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, i.e. those that resemble the adaptive gain model.</p>
<p>We note that the precise value of parameter <span class="math inline">\(k\)</span>, which interpolates between models implementing logistic and linear divisive normalization, is less important for fitting human data. This implies that the model is equally well fit with an exponentiated implementation of recurrent divisive normalization (<span class="math inline">\(k = 1\)</span>, such as the adaptive gain model) and with a linear implementation of recurrent divisive normalization (<span class="math inline">\(k\)</span> ~ <span class="math inline">\(0\)</span>), where inputs are transformed with a power parameter <span class="math inline">\(s\)</span> prior to normalization (<span class="math inline">\(s\neq 0\)</span>, see Eq. <a href="decoy.html#eq:decoy-power-eq">(2.11)</a>). This latter formulation of recurrent divisive normalization (as in Eq. <a href="decoy.html#eq:decoy-power-eq">(2.11)</a>) is conceptually analogous to a form of recurrent divisive normalization in which values are power transformed <span class="citation">(as in <a href="#ref-daviet2018" role="doc-biblioref">Daviet 2018</a>)</span>:
<span class="math display" id="eq:decoy-prdn">\[\begin{equation}
u_i(A_i)^{PRDN} = \frac{v(A_i)^{\alpha}}{v(A_i)^{\alpha}+v(avg_i^{ABD})^{\alpha}+c_i}
\tag{2.12}
\end{equation}\]</span></p>
<p>Indeed, Bayesian model selection reveals that this model fits the data equally well as the adaptive gain model (exceedance probability for PRDN = 0.49), suggesting that our dataset cannot arbitrate between an exponential and power transform of attribute values.</p>
<p>Why is the nonlinear compression of attribute values important? To explore the role of this procedure, we compared the maps of decoy influence produced by an implementation of the grandmother model incorporating a logistic (i.e. exponential) transform of attribute values (equivalent to the adaptive gain model) and an implementation of the grandmother model incorporating attribute values linearly (equivalent to recurrent divisive normalization without a power transform) in Fig. <a href="decoy.html#fig:decoy-nonlinearity">2.10</a>a. The nonlinearity qualitatively changes the form of the decoy map. The model featuring linearly coded attribute values produces stronger contextual effects for decoys of higher attribute values. This result can be traced back to the shape of the transducer function under linear attribute values (Fig. <a href="decoy.html#fig:decoy-nonlinearity">2.10</a>b, red curve), which is a non-symmetric concave function. Consequently, the curve deviates more from the identity line along the higher end of the abscissa and this leads to higher distortions in the translation of higher compared lower attribute values.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decoy-nonlinearity"></span>
<img src="figures/decoy-nonlinearity.png" alt="The role of the compressive nonlinearity. $\textbf{A:}$ Simulated decoy maps for the adaptive gain and recurrent divisive normalization models. It is noteworthy that the adaptive gain model (with bias terms $c_i=c_j=0$) produces symmetric regions of repulsion and attraction around the line of isopreference. By contrast, &quot;linear&quot; recurrent divisive normalization (i.e. RDN without a power transform applied to values prior to transduction) produces stronger repulsion and attraction for superior decoys (i.e. decoys above rather than below the isopreference line). $\textbf{B:}$ The asymmetric pattern of decoy influence seen for linear RDN occurs because the transducer is a (decelerating) Naka-Rushton function. This function is concave, meaning that the derivative is always higher (i.e. curve steeper) for low than high attribute values irrespective of the value of $x$. By contrast, the transducer for AG is sigmoidal and it is thus symmetric around the midpoint (which is itself adjusted to the context; this is the “adaptive” part). This panel plots the transductions applied to inputs by the Naka-Rushton function in red and by the Sigmoidal function in blue, with $v((avg)_i)=0.5$ (dashed line), $s=0.1$. The recurrent divisive normalization framework (without additional nonlinearity) implies that those attribute values which are always relatively smaller (closer to zero) are processed with higher gain. By contrast, the adaptive gain model implies that resources are allocated preferentially to the mean of a context, exaggerating binary distinctions which potentially straddle that midpoint (e.g. “low” vs. “high” value). $\textbf{C:}$ The relative strengths of the compromise, attraction, similarity and repulsion effects per the two decoy maps. $\textbf{D:}$ Illustration of the transfer function of the grandmother model, with $c_i=c_j=0$, $\beta_1=\beta_2=1$, and $v((avg)_i)=0.5$, across different values of $k$ and $s$. Note that with this parametrization, the transducer function is equivalent to a logistic function when $k=1$, and to a Naka-Rushton function when $k \sim 0$ and $s=1$. Power transforming inputs (by setting $s \neq 1$) in the Naka-Rushton definition approximates the sigmoidal shape of the adaptive gain transducer." width="100%" />
<p class="caption">
Figure 2.10: The role of the compressive nonlinearity. <span class="math inline">\(\textbf{A:}\)</span> Simulated decoy maps for the adaptive gain and recurrent divisive normalization models. It is noteworthy that the adaptive gain model (with bias terms <span class="math inline">\(c_i=c_j=0\)</span>) produces symmetric regions of repulsion and attraction around the line of isopreference. By contrast, “linear” recurrent divisive normalization (i.e. RDN without a power transform applied to values prior to transduction) produces stronger repulsion and attraction for superior decoys (i.e. decoys above rather than below the isopreference line). <span class="math inline">\(\textbf{B:}\)</span> The asymmetric pattern of decoy influence seen for linear RDN occurs because the transducer is a (decelerating) Naka-Rushton function. This function is concave, meaning that the derivative is always higher (i.e. curve steeper) for low than high attribute values irrespective of the value of <span class="math inline">\(x\)</span>. By contrast, the transducer for AG is sigmoidal and it is thus symmetric around the midpoint (which is itself adjusted to the context; this is the “adaptive” part). This panel plots the transductions applied to inputs by the Naka-Rushton function in red and by the Sigmoidal function in blue, with <span class="math inline">\(v((avg)_i)=0.5\)</span> (dashed line), <span class="math inline">\(s=0.1\)</span>. The recurrent divisive normalization framework (without additional nonlinearity) implies that those attribute values which are always relatively smaller (closer to zero) are processed with higher gain. By contrast, the adaptive gain model implies that resources are allocated preferentially to the mean of a context, exaggerating binary distinctions which potentially straddle that midpoint (e.g. “low” vs. “high” value). <span class="math inline">\(\textbf{C:}\)</span> The relative strengths of the compromise, attraction, similarity and repulsion effects per the two decoy maps. <span class="math inline">\(\textbf{D:}\)</span> Illustration of the transfer function of the grandmother model, with <span class="math inline">\(c_i=c_j=0\)</span>, <span class="math inline">\(\beta_1=\beta_2=1\)</span>, and <span class="math inline">\(v((avg)_i)=0.5\)</span>, across different values of <span class="math inline">\(k\)</span> and <span class="math inline">\(s\)</span>. Note that with this parametrization, the transducer function is equivalent to a logistic function when <span class="math inline">\(k=1\)</span>, and to a Naka-Rushton function when <span class="math inline">\(k \sim 0\)</span> and <span class="math inline">\(s=1\)</span>. Power transforming inputs (by setting <span class="math inline">\(s \neq 1\)</span>) in the Naka-Rushton definition approximates the sigmoidal shape of the adaptive gain transducer.
</p>
</div>
<p>This is in contrast to what happens in the presence of an exponential nonlinearity as in the adaptive gain model. This compression of attribute values produces an S-shaped transducer (Fig. <a href="decoy.html#fig:decoy-nonlinearity">2.10</a>b, blue curve), akin to the sigmoidal shapes of response functions ubiquitous in neural coding. Notably, this transducer is symmetric around the inflection point (which corresponds to the mean of attribute values of the context when <span class="math inline">\(c_i=0\)</span>) and thus results in equal levels of distortion across the low and high end of attribute space. The bias term <span class="math inline">\(c_i\)</span> can tip the balance in either direction: it can produce stronger effects for higher attribute values when <span class="math inline">\(c_i&gt;0\)</span> and stronger effects for lower attribute values when <span class="math inline">\(c_i&lt;0\)</span> (see Fig. <a href="decoy.html#fig:decoy-sim">2.6</a>d for simulations). Of note, the sigmoidal shape of the transducer here can be approximated by power transforming inputs prior to normalizing them in the recurrent divisive normalization scheme (i.e. <span class="math inline">\(s&gt;0\)</span> and <span class="math inline">\(k\sim 0\)</span>, Fig. <a href="decoy.html#fig:decoy-nonlinearity">2.10</a>d lightest blue curve). This result helps clarify why this operation can capture the human data equally well as the exponential nonlinearity in the adaptive gain model.</p>
<p>Finally, we asked whether the adaptive gain model fits better than the full grandmother model after appropriate penalization for complexity. A failure to do so would imply the existence of a “hybrid” normalization solution that fits the human data even better, presumably involving some combination of parameters that has yet to be described in the literature. To asses this, we performed Bayesian model selection on complexity-penalized model fit metrics (Bayesian Information Criterion, BIC) which revealed that the exceedance probability for the normalization scheme favored by our empirical data, the adaptive gain model, over the grandmother model is 0.97, offering evidence against a hybrid solution.</p>
</div>
</div>
</div>
<div id="interim-discussion-1" class="section level3" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Interim Discussion</h3>
<p>Altogether, the results of our modeling suggest that a very simple computational scheme appealing to the relative encoding of attribute values may capture the full pattern of decoy influence in our dataset. We identified a 5-parameter model, previously termed adaptive gain, which fit the data exceptionally well, both at a fine scale (at the level of effects in individual participants) and on the aggregate level (the average map of decoy influence). We compared this model to related theoretical accounts in the literature, which appeal to a contextual normalization of attribute values, first, via conventional model selection techniques and second, through a comprehensive model comparison exercise for which we devised a flexible “grandmother” model encompassing a wide space of putative normalization schemes. The results of our computational work indicated that the rich pattern of decoy influence is best accounted for by information processing schemes that compressively transduce inputs, appealing to normalization by the central tendency of context and recurrently overweighing the contribution of the target input to the normalization.</p>
</div>
</div>
<div id="discussion" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Discussion</h2>
<p>Decoy effects have been studied for decades, but substantial controversy has surrounded their replicability, interrelationship, and computational origins. This chapter sheds new light on these debates by gathering and analyzing a large-scale dataset that systematically maps the influence of a decoy stimulus across both the inferior and superior locations of multiattribute space. Conducting our analysis in a conventional fashion, we broadly replicate past studies, in that we find strong attraction effects, strong compromise effects, and a weaker and more variable similarity effect (not significant in our dataset). As in previous studies, the three decoy effects are correlated across the cohort, with a positive relationship observed between attraction and compromise, and a negative relationship between those two and the strength of the similarity effect <span class="citation">(<a href="#ref-berkowitsch2014" role="doc-biblioref">Berkowitsch, Scheibehenne, and Rieskamp 2014</a>)</span>. This finding implies that three decoy phenomena have a single cause, and indeed previous dynamic models (in which information is accumulated over time) have been able to capture the three discrete effects with a single set of parameters <span class="citation">(<a href="#ref-roe2001" role="doc-biblioref">Roe, Busemeyer, and Townsend 2001</a>; <a href="#ref-usher2004" role="doc-biblioref">Usher and McClelland 2004</a>; <a href="#ref-bhatia2013" role="doc-biblioref">Bhatia 2013</a>; <a href="#ref-trueblood2014" role="doc-biblioref">Trueblood, Brown, and Heathcote 2014</a>; <a href="#ref-bhui2018" role="doc-biblioref">Bhui and Gershman 2018</a>)</span>. Here, we use dimensionality reduction on the full decoy influence map to confirm that indeed, there is a single component that explains the vast majority (~95%) of the variance in decoy influence, suggestive of a single computational origin for these biases.</p>
<p>To understand the computational origin of decoy effects, we chose to model our data with a framework based on divisive normalization. We made this choice because the normalization model offers a simple, parsimonious account of contextual biases in decision-making based on a rich, neurobiologically grounded tradition in the cognitive sciences <span class="citation">(<a href="#ref-carandini2012" role="doc-biblioref">Carandini and Heeger 2012</a>; <a href="#ref-louie2013" role="doc-biblioref">Louie, Khaw, and Glimcher 2013</a>; <a href="#ref-rigoli2019" role="doc-biblioref">Rigoli 2019</a>; <a href="#ref-landry2021" role="doc-biblioref">Landry and Webb 2021</a>; <a href="#ref-webb2021" role="doc-biblioref">Webb, Glimcher, and Louie 2021</a>)</span>. In particular, it allowed us to systematically measure the influence of various candidate computational steps on the predicted decoy map, providing an interpretable mapping from model to data (Fig. <a href="decoy.html#fig:decoy-sim">2.6</a>). On this basis, we were able to establish (for example) that normalization occurs relative to the average of the available values (via a sigmoidal gain function) rather than to the lower end (via a concave gain function) as proposed in some previous models (Fig. <a href="decoy.html#fig:decoy-nonlinearity">2.10</a>). This characteristic sigmoidal shape of the transfer function may be approximated by transforming inputs via a power term (<span class="math inline">\(\alpha&gt;1\)</span>) in recurrent divisive normalization <span class="citation">(<a href="#ref-daviet2018" role="doc-biblioref">Daviet 2018</a>; <a href="#ref-webb2021" role="doc-biblioref">Webb, Glimcher, and Louie 2021</a>)</span>.</p>
<p>Overall, out of the models tested here, evidence favored a model that has previously been described in the literature as the <em>adaptive gain model</em> <span class="citation">(<a href="#ref-cheadle2014" role="doc-biblioref">Cheadle et al. 2014</a>; <a href="#ref-li2018" role="doc-biblioref">V. Li et al. 2018</a>)</span>. This account is closely related to other models involving recurrent divisive normalization, especially those proposing that values are nonlinearly transformed beforehand, as well as being very similar to another model known as the logistic model of subjective value <span class="citation">(<a href="#ref-rigoli2019" role="doc-biblioref">Rigoli 2019</a>)</span>. There is a close correspondence between qualitative features of model-simulated and human performance displayed in Fig. <a href="decoy.html#fig:decoy-map">2.4</a>, and in particular, a close correspondence achieved after decomposition of the decoy map into linear components using singular value decomposition (Fig. <a href="decoy.html#fig:decoy-svd">2.5</a>). The adaptive gain model even predicted a new and potentially counter-intuitive relationship between the decoy effects: that when the compromise effect is positive, attraction should dominate over repulsion (and vice versa), a prediction that was satisfied in the data.</p>
<p>Under the adaptive gain control framework described here, decoy effects occur because of contextual biases arising when each target item is transduced via a logistic function whose inflection point lies at the mean of all three items including the decoy. For example, the “attraction” effect thus occurs because when the decoy is lower in value than item A, the inflection point is lower than item A, and so A lies at the steepest portion of the sigmoidal gain function and is thus “overvalued” or repulsed away from this mean point. The precise converse occurs when the decoy is higher in value than A, as well as for B. Previous work has demonstrated that exactly this mechanism can in principle account for a range of decision biases arising in the presence of distractors, across perceptual, cognitive and economic domains <span class="citation">(<a href="#ref-li2018" role="doc-biblioref">V. Li et al. 2018</a>)</span>. The brain may have evolved the type of normalization scheme proposed here because it promotes efficient neural coding <span class="citation">(<a href="#ref-summerfield2015" role="doc-biblioref">Summerfield and Tsetsos 2015</a>, <a href="#ref-summerfield2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>Whereas the attraction effect tends to be highly robust and consistent across participants, the strength and directionality of the compromise effect and similarity effects tend to be more idiosyncratic. Indeed, the similarity effect did not reach statistical significance in our dataset. In our model, the compromise and similarity effects occur when attractive and repulsive processes are asymmetric due to differential weighing or biasing of the two attributes, causing attraction effects (and their converse for superior decoys) to warp and/or “spill over” into locations where compromise and similarity decoys are typically tested. In other words, the fragile nature of the compromise and similarity effects might be at least in part due to heterogeneity in the asymmetric way each attribute is coded or transformed, which in turn might (for example) be due to differing choice concerning stimulus materials. A systematic unpicking of ways in which different classes of stimulus material (e.g. numerical values in distinct ranges, perceptual stimuli such as rectangles, and vignettes) are encoded, and thus why decoy effects may or may not have emerged in previous studies <span class="citation">(<a href="#ref-frederick2014" role="doc-biblioref">Frederick, Lee, and Baskin 2014</a>; <a href="#ref-huber2014" role="doc-biblioref">Joel Huber, Payne, and Puto 2014</a>; <a href="#ref-spektor2021" role="doc-biblioref">Spektor, Bhatia, and Gluth 2021</a>)</span>, is beyond the scope of our research project here. However, our simulations suggest that a relatively low-dimensional encoding model may be sufficient to capture this variation and thus to pinpoint the source of variation in previous studies.</p>
</div>
<div id="conclusion" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Conclusion</h2>
<p>Adding a “decoy” alternative to a choice set can shift preferences in multialternative economic decisions. A long research tradition focusing on this contextual bias has provided evidence for three classic decoy effects – the compromise, attraction and similarity effects – but their replicability, interrelationship and computational origins have sparked controversy. To address these, we carried out a large-scale incentive-compatible study, charting the full two dimensional attribute space of decoy effects. Our analyses indicated that a single explanatory factor can account for the rich pattern of decoy influence we observe. Indeed, a simple process model, appealing to the principle of divisive normalization, can capture our results both at the individual and aggregate level.</p>
</div>
<div id="acknowledgements" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Acknowledgements</h2>
<p>This chapter is based on the published work <em>Dumbalska, T., Li, V., Tsetsos, K., &amp; Summerfield, C. (2020). A map of decoy influence in human multialternative choice. Proceedings of the National Academy of Sciences, 117(40), 25169-25178.</em></p>
<p>The human data for this chapter was procured by Dr Vickie Li. Dr Vickie Li and Prof Christopher Summerfield jointly designed and Dr Vickie Li built the experiment and administered data collection. Dr Konstantinos Tsetsos provided feedback on code for the modeling simulations.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-berkowitsch2014" class="csl-entry">
Berkowitsch, N. A. J., B. Scheibehenne, and J. Rieskamp. 2014. <span>“Rigorously Testing Multialternative Decision Field Theory Against Random Utility Models.”</span> <em>Journal of Experimental Psychology: General</em> 143 (3): 1331–48. <a href="https://doi.org/10.1037/a0035159">https://doi.org/10.1037/a0035159</a>.
</div>
<div id="ref-bhatia2013" class="csl-entry">
Bhatia, S. 2013. <span>“Associations and the Accumulation of Preference.”</span> <em>Psychological Review</em> 120 (3): 522–43. <a href="https://doi.org/10.1037/a0032457">https://doi.org/10.1037/a0032457</a>.
</div>
<div id="ref-bhui2018" class="csl-entry">
Bhui, R., and S. J. Gershman. 2018. <span>“Decision by Sampling Implements Efficient Coding of Psychoeconomic Functions.”</span> <em>Psychological Review</em> 125 (6): 985–1001. <a href="https://doi.org/10.1037/rev0000123">https://doi.org/10.1037/rev0000123</a>.
</div>
<div id="ref-block1960" class="csl-entry">
Block, H. D., and J. Marschak. 1960. <span>“Random Orderings and Stochastic Theories of Responses.”</span> <em>Contributions to Probability and Statistics</em>, 97–132.
</div>
<div id="ref-bushong2021" class="csl-entry">
Bushong, Benjamin, Matthew Rabin, and Joshua Schwartzstein. 2021. <span>“A Model of Relative Thinking.”</span> <em>The Review of Economic Studies</em> 88 (1): 162–91. <a href="https://doi.org/10.1093/restud/rdaa055">https://doi.org/10.1093/restud/rdaa055</a>.
</div>
<div id="ref-cao2017" class="csl-entry">
Cao, Yanshuai, and Luyu Wang. 2017. <span>“Automatic Selection of t-<span>SNE</span> Perplexity.”</span> <em><span>arXiv</span>:1708.03229 [Cs, Stat]</em>, August. <a href="http://arxiv.org/abs/1708.03229">http://arxiv.org/abs/1708.03229</a>.
</div>
<div id="ref-carandini2012" class="csl-entry">
Carandini, M., and D. J. Heeger. 2012. <span>“Normalization as a Canonical Neural Computation.”</span> <em>Nature Reviews Neuroscience</em> 13 (1): 51–62. <a href="https://doi.org/10.1038/nrn3136">https://doi.org/10.1038/nrn3136</a>.
</div>
<div id="ref-cheadle2014" class="csl-entry">
Cheadle, S., V. Wyart, Konstantinos Tsetsos, Nicholas Myers, Vincent de Gardelle, Santiago Herce Castañón, and Christopher Summerfield. 2014. <span>“Adaptive Gain Control During Human Perceptual Choice.”</span> <em>Neuron</em> 81 (6): 1429–41. <a href="https://doi.org/10.1016/j.neuron.2014.01.020">https://doi.org/10.1016/j.neuron.2014.01.020</a>.
</div>
<div id="ref-daviet2018" class="csl-entry">
Daviet, R. 2018. <span>“Methods for Statistical Analysis and Prediction of Discrete Choices.”</span> <em><span>PhD</span>/Doctoral Dissertation</em>.
</div>
<div id="ref-frederick2014" class="csl-entry">
Frederick, S., L. Lee, and E. Baskin. 2014. <span>“The Limits of Attraction.”</span> <em>Journal of Marketing Research</em> 51 (4): 487–507. <a href="https://doi.org/10.1509/jmr.12.0061">https://doi.org/10.1509/jmr.12.0061</a>.
</div>
<div id="ref-hotaling2010" class="csl-entry">
Hotaling, J. M., J. R. Busemeyer, and J. Li. 2010. <span>“Theoretical Developments in Decision Field Theory: Comment on Tsetsos, Usher, and Chater (2010).”</span> <em>Psychological Review</em> 117 (4): 1294–98. <a href="https://doi.org/10.1037/a0020401">https://doi.org/10.1037/a0020401</a>.
</div>
<div id="ref-huber2014" class="csl-entry">
Huber, Joel, John W Payne, and Christopher P Puto. 2014. <span>“Let’s Be Honest about the Attraction Effect.”</span> <em>Journal of Marketing Research</em> 51 (4): 520–25.
</div>
<div id="ref-huber1982" class="csl-entry">
Huber, J., J. W. Payne, and C. P. Puto. 1982. <span>“Adding Asymmetrically Dominated Alternatives: Violations of Regularity and the Similarity Hypothesis.”</span> <em>Journal of Consumer Research</em> 9 (1): 90. <a href="https://doi.org/10.1086/208899">https://doi.org/10.1086/208899</a>.
</div>
<div id="ref-josiam1995" class="csl-entry">
Josiam, Bharath M, and JS Perry Hobson. 1995. <span>“Consumer Choice in Context: The Decoy Effect in Travel and Tourism.”</span> <em>Journal of Travel Research</em> 34 (1): 45–50.
</div>
<div id="ref-landry2021" class="csl-entry">
Landry, Peter, and Ryan Webb. 2021. <span>“Pairwise Normalization: A Neuroeconomic Theory of Multi-Attribute Choice.”</span> <em>Journal of Economic Theory</em> 193 (April): 105221. <a href="https://doi.org/10.1016/j.jet.2021.105221">https://doi.org/10.1016/j.jet.2021.105221</a>.
</div>
<div id="ref-latty2011" class="csl-entry">
Latty, T., and M. Beekman. 2011. <span>“Irrational Decision-Making in an Amoeboid Organism: Transitivity and Context-Dependent Preferences.”</span> <em>Proceedings of the Royal Society B: Biological Sciences</em> 278 (1703): 307–12. <a href="https://doi.org/10.1098/rspb.2010.1045">https://doi.org/10.1098/rspb.2010.1045</a>.
</div>
<div id="ref-lea2015" class="csl-entry">
Lea, A. M., and M. J. Ryan. 2015. <span>“Irrationality in Mate Choice Revealed by Túngara Frogs.”</span> <em>Science</em> 349 (6251): 964–66. <a href="https://doi.org/10.1126/science.aab2012">https://doi.org/10.1126/science.aab2012</a>.
</div>
<div id="ref-li2018" class="csl-entry">
Li, Vickie, Elizabeth Michael, Jan Balaguer, Santiago Herce Castañón, and Christopher Summerfield. 2018. <span>“Gain Control Explains the Effect of Distraction in Human Perceptual, Cognitive, and Economic Decision Making.”</span> <em>Proceedings of the National Academy of Sciences</em> 115 (38): E8825–34. <a href="https://doi.org/10.1073/pnas.1805224115">https://doi.org/10.1073/pnas.1805224115</a>.
</div>
<div id="ref-louie2013" class="csl-entry">
Louie, K., M. W. Khaw, and P. W. Glimcher. 2013. <span>“Normalization Is a General Neural Mechanism for Context-Dependent Decision Making.”</span> <em>Proceedings of the National Academy of Sciences</em> 110 (15): 6139–44. <a href="https://doi.org/10.1073/pnas.1217854110">https://doi.org/10.1073/pnas.1217854110</a>.
</div>
<div id="ref-natenzon2019" class="csl-entry">
Natenzon, P. 2019. <span>“Random Choice and Learning.”</span> <em>Journal of Political Economy</em> 127 (1): 419–57. <a href="https://doi.org/10.1086/700762">https://doi.org/10.1086/700762</a>.
</div>
<div id="ref-noguchi2014" class="csl-entry">
Noguchi, T., and N. Stewart. 2014. <span>“In the Attraction, Compromise, and Similarity Effects, Alternatives Are Repeatedly Compared in Pairs on Single Dimensions.”</span> <em>Cognition</em> 132 (1): 44–56. <a href="https://doi.org/10.1016/j.cognition.2014.03.006">https://doi.org/10.1016/j.cognition.2014.03.006</a>.
</div>
<div id="ref-parrish2015" class="csl-entry">
Parrish, A. E., T. A. Evans, and M. J. Beran. 2015. <span>“Rhesus Macaques (Macaca Mulatta) Exhibit the Decoy Effect in a Perceptual Discrimination Task.”</span> <em>Attention, Perception, and Psychophysics</em> 77 (5): 1715–25. <a href="https://doi.org/10.3758/s13414-015-0885-6">https://doi.org/10.3758/s13414-015-0885-6</a>.
</div>
<div id="ref-rieskamp2006" class="csl-entry">
Rieskamp, Jörg, Jerome R Busemeyer, and Barbara A Mellers. 2006. <span>“Extending the Bounds of Rationality: Evidence and Theories of Preferential Choice.”</span> <em>Journal of Economic Literature</em> 44 (3): 631–61. <a href="https://doi.org/10.1257/jel.44.3.631">https://doi.org/10.1257/jel.44.3.631</a>.
</div>
<div id="ref-rigoli2019" class="csl-entry">
Rigoli, Francesco. 2019. <span>“Reference Effects on Decision-Making Elicited by Previous Rewards.”</span> <em>Cognition</em> 192 (November): 104034. <a href="https://doi.org/10.1016/j.cognition.2019.104034">https://doi.org/10.1016/j.cognition.2019.104034</a>.
</div>
<div id="ref-rigoli2017" class="csl-entry">
Rigoli, Francesco, Christoph Mathys, Karl J. Friston, and Raymond J. Dolan. 2017. <span>“A Unifying Bayesian Account of Contextual Effects in Value-Based Choice.”</span> <em><span>PLOS</span> Computational Biology</em> 13 (10): e1005769.
</div>
<div id="ref-roe2001" class="csl-entry">
Roe, R. M., J. R. Busemeyer, and J. T. Townsend. 2001. <span>“Multialternative Decision Field Theory: A Dynamic Connectionist Model of Decision Making.”</span> <em>Psychological Review</em> 108 (2): 370–92. <a href="https://doi.org/10.1037/0033-295X.108.2.370">https://doi.org/10.1037/0033-295X.108.2.370</a>.
</div>
<div id="ref-shafir1994" class="csl-entry">
Shafir, S. 1994. <span>“Intransitivity of Preferences in Honey Bees: Support for ’Comparative’ Evaluation of Foraging Options.”</span> <em>Animal Behaviour</em> 48 (1): 55–67. <a href="https://doi.org/10.1006/anbe.1994.1211">https://doi.org/10.1006/anbe.1994.1211</a>.
</div>
<div id="ref-simonson1989" class="csl-entry">
Simonson, Itamar. 1989. <span>“Choice Based on Reasons: The Case of Attraction and Compromise Effects.”</span> <em>Journal of Consumer Research</em> 16 (2): 158. <a href="https://doi.org/10.1086/209205">https://doi.org/10.1086/209205</a>.
</div>
<div id="ref-soltani2012" class="csl-entry">
Soltani, Alireza, Benedetto De Martino, and Colin Camerer. 2012. <span>“A Range-Normalization Model of Context-Dependent Choice: A New Model and Evidence.”</span> Edited by Laurence T. Maloney. <em><span>PLoS</span> Computational Biology</em> 8 (7): e1002607. <a href="https://doi.org/10.1371/journal.pcbi.1002607">https://doi.org/10.1371/journal.pcbi.1002607</a>.
</div>
<div id="ref-spektor2021" class="csl-entry">
Spektor, Mikhail S, Sudeep Bhatia, and Sebastian Gluth. 2021. <span>“The Elusiveness of Context Effects in Decision Making.”</span> <em>Trends in Cognitive Sciences</em>.
</div>
<div id="ref-stephan2009" class="csl-entry">
Stephan, Klaas Enno, Will D. Penny, Jean Daunizeau, Rosalyn J. Moran, and Karl J. Friston. 2009. <span>“Bayesian Model Selection for Group Studies.”</span> <em><span>NeuroImage</span></em> 46 (4): 1004–17. <a href="https://doi.org/10.1016/j.neuroimage.2009.03.025">https://doi.org/10.1016/j.neuroimage.2009.03.025</a>.
</div>
<div id="ref-summerfield2015" class="csl-entry">
———. 2015. <span>“Do Humans Make Good Decisions?”</span> <em>Trends in Cognitive Sciences</em> 19 (1): 27–34. <a href="https://doi.org/10.1016/j.tics.2014.11.005">https://doi.org/10.1016/j.tics.2014.11.005</a>.
</div>
<div id="ref-summerfield2020" class="csl-entry">
———. 2020. <span>“Rationality and Efficiency in Human Decision-Making.”</span> <em>The Cognitive Neurosciences</em>, 427.
</div>
<div id="ref-trueblood2012" class="csl-entry">
Trueblood, J. S. 2012. <span>“Multialternative Context Effects Obtained Using an Inference Task.”</span> <em>Psychonomic Bulletin and Review</em> 19 (5): 962–68. <a href="https://doi.org/10.3758/s13423-012-0288-9">https://doi.org/10.3758/s13423-012-0288-9</a>.
</div>
<div id="ref-trueblood2014" class="csl-entry">
Trueblood, J. S., S. D. Brown, and A. Heathcote. 2014. <span>“The Multiattribute Linear Ballistic Accumulator Model of Context Effects in Multialternative Choice.”</span> <em>Psychological Review</em> 121 (2): 179–205. <a href="https://doi.org/10.1037/a0036137">https://doi.org/10.1037/a0036137</a>.
</div>
<div id="ref-trueblood2015" class="csl-entry">
———. 2015. <span>“The Fragile Nature of Contextual Preference Reversals: Reply to Tsetsos, Chater, and Usher (2015).”</span> <em>Psychological Review</em> 122 (4): 848–53. <a href="https://doi.org/10.1037/a0039656">https://doi.org/10.1037/a0039656</a>.
</div>
<div id="ref-trueblood2013" class="csl-entry">
Trueblood, J. S., S. D. Brown, A. Heathcote, and J. R. Busemeyer. 2013. <span>“Not Just for Consumers: Context Effects Are Fundamental to Decision Making.”</span> <em>Psychological Science</em> 24 (6): 901–8. <a href="https://doi.org/10.1177/0956797612464241">https://doi.org/10.1177/0956797612464241</a>.
</div>
<div id="ref-tsetsos2012" class="csl-entry">
Tsetsos, Konstantinos, N. Chater, and M. Usher. 2012. <span>“Salience Driven Value Integration Explains Decision Biases and Preference Reversal.”</span> <em>Proceedings of the National Academy of Sciences of the United States of America</em> 109 (24): 9659–64. <a href="https://doi.org/10.1073/pnas.1119569109">https://doi.org/10.1073/pnas.1119569109</a>.
</div>
<div id="ref-tsetsos2010" class="csl-entry">
Tsetsos, K., M. Usher, and N. Chater. 2010. <span>“Preference Reversal in Multiattribute Choice.”</span> <em>Psychological Review</em> 117 (4): 1275–91. <a href="https://doi.org/10.1037/a0020580">https://doi.org/10.1037/a0020580</a>.
</div>
<div id="ref-turner2018" class="csl-entry">
Turner, B. M., D. R. Schley, C. Muller, and K. Tsetsos. 2018. <span>“Competing Theories of Multialternative, Multiattribute Preferential Choice.”</span> <em>Psychological Review</em> 125 (3): 329–62. <a href="https://doi.org/10.1037/rev0000089">https://doi.org/10.1037/rev0000089</a>.
</div>
<div id="ref-tversky1972a" class="csl-entry">
Tversky, A. 1972. <span>“Elimination by Aspects: A Theory of Choice.”</span> <em>Psychological Review</em> 79 (4): 281–99. <a href="https://doi.org/10.1037/h0032955">https://doi.org/10.1037/h0032955</a>.
</div>
<div id="ref-usher2004" class="csl-entry">
Usher, M., and J. L. McClelland. 2004. <span>“Loss Aversion and Inhibition in Dynamical Models of Multialternative Choice.”</span> <em>Psychological Review</em> 111 (3): 757–69. <a href="https://doi.org/10.1037/0033-295X.111.3.757">https://doi.org/10.1037/0033-295X.111.3.757</a>.
</div>
<div id="ref-vandermaaten2008" class="csl-entry">
Van Der Maaten, L., and G. Hinton. 2008. <span>“Visualizing Data Using t-<span>SNE</span>.”</span> <em>Journal of Machine Learning Research</em> 9: 2579–2625.
</div>
<div id="ref-webb2021" class="csl-entry">
Webb, R, P. W. Glimcher, and K. Louie. 2021. <span>“The Normalization of Consumer Valuations: Context-Dependent Preferences from Neurobiological Constraints.”</span> <em>Manag. Sci.</em>
</div>
<div id="ref-yang2014" class="csl-entry">
Yang, S., and M. Lynn. 2014. <span>“More Evidence Challenging the Robustness and Usefulness of the Attraction Effect.”</span> <em>Journal of Marketing Research</em> 51 (4): 508–13. <a href="https://doi.org/10.1509/jmr.14.0020">https://doi.org/10.1509/jmr.14.0020</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ulyngs/oxforddown/tree/master/02-decoy.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
